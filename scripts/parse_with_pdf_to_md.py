#!/usr/bin/env python3
"""
–ü–æ–ª–Ω—ã–π pipeline: –ø–∞—Ä—Å–∏–Ω–≥ + —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF + –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ MD.

–≠—Ç–∞–ø—ã:
1. –ü–∞—Ä—Å–∏–Ω–≥ 40 —Å—Ç—Ä–∞–Ω–∏—Ü —è–Ω–≤–∞—Ä—è 2024
2. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ 100 PDF —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤
3. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PDF –≤ MD (–¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞)
4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
"""

import asyncio
import json
import random
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Any

import httpx
from structlog import get_logger

from src.scraper.playwright_scraper import PlaywrightScraper

logger = get_logger(__name__)

# –ü–æ–ø—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å pdfplumber –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
try:
    import pdfplumber
    PDF_SUPPORT = True
except ImportError:
    print("‚ö†Ô∏è  pdfplumber –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. PDF‚ÜíMD –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–∞.")
    print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pdfplumber")
    PDF_SUPPORT = False


def pdf_to_markdown(pdf_path: Path) -> str:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å PDF –≤ Markdown.

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤ MD.
    """
    if not PDF_SUPPORT:
        return ""

    try:
        with pdfplumber.open(pdf_path) as pdf:
            pages_text = []
            for i, page in enumerate(pdf.pages, 1):
                text = page.extract_text()
                if text:
                    pages_text.append(f"# –°—Ç—Ä–∞–Ω–∏—Ü–∞ {i}\n\n{text}\n")

            return "\n---\n\n".join(pages_text)
    except Exception as e:
        logger.error("pdf_conversion_failed", path=str(pdf_path), error=str(e))
        return ""


def calculate_pdf_hash(pdf_bytes: bytes) -> str:
    """–í—ã—á–∏—Å–ª–∏—Ç—å SHA256 hash PDF –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏."""
    return hashlib.sha256(pdf_bytes).hexdigest()


async def parse_and_download():
    """–ü–æ–ª–Ω—ã–π pipeline –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è."""

    print("=" * 80)
    print("üöÄ –ü–û–õ–ù–´–ô PIPELINE: –ü–ê–†–°–ò–ù–ì + PDF + MD –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø")
    print("=" * 80)
    print()

    # –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)

    pdfs_dir = data_dir / "january_2024_pdfs"
    pdfs_dir.mkdir(exist_ok=True)

    mds_dir = data_dir / "january_2024_mds"
    mds_dir.mkdir(exist_ok=True)

    # –ó–∞–ø—É—Å–∫ –±–µ–∑ CDP (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞—Ö)
    print("üåê –ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞ (–±–µ–∑ CDP –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)...")
    async with PlaywrightScraper(use_cdp=False) as scraper:
        print("‚úÖ –ë—Ä–∞—É–∑–µ—Ä –∑–∞–ø—É—â–µ–Ω\n")

        # ============================================================
        # –®–ê–ì 1: –ü–∞—Ä—Å–∏–Ω–≥ 40 —Å—Ç—Ä–∞–Ω–∏—Ü
        # ============================================================

        print("=" * 80)
        print("–®–ê–ì 1: –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö 40 —Å—Ç—Ä–∞–Ω–∏—Ü (—è–Ω–≤–∞—Ä—å 2024)")
        print("=" * 80)
        print()

        # –û—Ç–∫—Ä—ã—Ç—å –ö–ê–î –ê—Ä–±–∏—Ç—Ä
        await scraper.page.goto("https://kad.arbitr.ru", wait_until="networkidle")
        await asyncio.sleep(2)

        # –ó–∞–∫—Ä—ã—Ç—å popup
        try:
            await scraper.page.keyboard.press("Escape")
            await asyncio.sleep(1)
        except Exception:
            pass

        # –ó–∞–ø–æ–ª–Ω–∏—Ç—å —Ñ–æ—Ä–º—É
        date_inputs = await scraper.page.query_selector_all('input[placeholder="–¥–¥.–º–º.–≥–≥–≥–≥"]')
        if len(date_inputs) >= 2:
            await date_inputs[0].click()
            await asyncio.sleep(0.2)
            await date_inputs[0].fill("01.01.2024")
            await asyncio.sleep(0.5)

            await date_inputs[1].click()
            await asyncio.sleep(0.2)
            await date_inputs[1].fill("31.01.2024")
            await asyncio.sleep(0.5)

        await scraper.page.click("body")
        await asyncio.sleep(0.5)

        # –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–æ—Ä–º—É
        await scraper.page.click("#b-form-submit")
        await asyncio.sleep(5)

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        total_pages_input = await scraper.page.query_selector("input#documentsPagesCount")
        if not total_pages_input:
            print("‚ùå –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return

        total_pages_str = await total_pages_input.get_attribute("value")
        total_pages = int(total_pages_str) if total_pages_str else 0

        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
        print(f"üìÑ –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å: –í–°–ï {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
        print()

        # –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        all_cases = []
        start_time = datetime.now()

        for page_num in range(1, total_pages + 1):
            print(f"üìñ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}/{total_pages}...", end=" ")

            try:
                cases = await scraper._parse_current_page()
                all_cases.extend(cases)
                print(f"‚úì {len(cases)} –¥–µ–ª (–≤—Å–µ–≥–æ: {len(all_cases)})")

                # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 —Å—Ç—Ä–∞–Ω–∏—Ü
                if page_num % 10 == 0:
                    temp_file = data_dir / f"january_2024_cases_page_{page_num}.json"
                    temp_file.write_text(
                        json.dumps(all_cases, ensure_ascii=False, indent=2),
                        encoding="utf-8"
                    )
                    print(f"   üíæ Checkpoint: {len(all_cases)} –¥–µ–ª")

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                continue

            # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            if page_num < total_pages:
                try:
                    link = await scraper.page.query_selector(f'a[href="#page{page_num + 1}"]')
                    if link:
                        await link.click()
                        await asyncio.sleep(5)
                    else:
                        print(f"   ‚ö†Ô∏è  –°—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num + 1} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                        break
                except Exception as e:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏: {e}")
                    break

        parsing_time = (datetime.now() - start_time).total_seconds()

        print()
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(all_cases)} –¥–µ–ª –∑–∞ {parsing_time:.1f} —Å–µ–∫")
        print()

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ –¥–µ–ª–∞
        cases_file = data_dir / "january_2024_cases.json"
        cases_file.write_text(
            json.dumps(all_cases, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {cases_file}")
        print()

        # ============================================================
        # –®–ê–ì 2: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ 100 PDF
        # ============================================================

        print("=" * 80)
        print("–®–ê–ì 2: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ 100 PDF —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤")
        print("=" * 80)
        print()

        NUM_PDFS = min(100, len(all_cases))
        selected_cases = random.sample(all_cases, NUM_PDFS)

        print(f"üìã –í—ã–±—Ä–∞–Ω–æ –¥–µ–ª –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {NUM_PDFS}")
        print()

        downloaded_count = 0
        converted_count = 0
        download_errors = []
        pdf_metadata = []

        for i, case in enumerate(selected_cases, 1):
            print(f"üìÑ [{i}/{NUM_PDFS}] {case['case_number']}")

            try:
                # –û—Ç–∫—Ä—ã—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–µ–ª–∞
                case_url = case['url']
                if not case_url.startswith('http'):
                    case_url = f"https://kad.arbitr.ru{case_url}"

                await scraper.page.goto(case_url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(2)

                # –ù–∞–π—Ç–∏ PDF —Å—Å—ã–ª–∫–∏
                doc_links = await scraper.page.query_selector_all('a[href$=".pdf"]')

                if not doc_links:
                    print(f"   ‚ö†Ô∏è  PDF –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    download_errors.append({
                        "case_number": case["case_number"],
                        "error": "no_pdf_links"
                    })
                    continue

                # –í–∑—è—Ç—å –ø–µ—Ä–≤—É—é —Å—Å—ã–ª–∫—É
                first_link = doc_links[0]
                link_text = await first_link.inner_text()
                pdf_url = await first_link.get_attribute("href")

                # –°–∫–∞—á–∞—Ç—å PDF
                cookies = await scraper.page.context.cookies()
                cookie_dict = {cookie['name']: cookie['value'] for cookie in cookies}

                async with httpx.AsyncClient(
                    cookies=cookie_dict,
                    timeout=30.0,
                    follow_redirects=True
                ) as client:
                    response = await client.get(pdf_url)

                    if response.status_code == 200:
                        content_type = response.headers.get('content-type', '')

                        if 'pdf' in content_type.lower() or pdf_url.endswith('.pdf'):
                            pdf_content = response.content
                            pdf_hash = calculate_pdf_hash(pdf_content)

                            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å PDF
                            filename = f"{case['case_number'].replace('/', '_')}.pdf"
                            pdf_path = pdfs_dir / filename
                            pdf_path.write_bytes(pdf_content)

                            print(f"   ‚úÖ PDF: {len(pdf_content)//1024} KB")
                            downloaded_count += 1

                            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ MD
                            if PDF_SUPPORT:
                                md_content = pdf_to_markdown(pdf_path)
                                if md_content:
                                    md_filename = filename.replace('.pdf', '.md')
                                    md_path = mds_dir / md_filename
                                    md_path.write_text(md_content, encoding='utf-8')

                                    # –≠–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞
                                    md_size = len(md_content.encode('utf-8'))
                                    savings_pct = (1 - md_size / len(pdf_content)) * 100

                                    print(f"   ‚úÖ MD: {md_size//1024} KB (—ç–∫–æ–Ω–æ–º–∏—è: {savings_pct:.0f}%)")
                                    converted_count += 1

                            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                            pdf_metadata.append({
                                "case_number": case["case_number"],
                                "case_date": case["case_date"],
                                "judge": case.get("judge"),
                                "court": case.get("court"),
                                "pdf_filename": filename,
                                "pdf_size": len(pdf_content),
                                "pdf_hash": pdf_hash,
                                "pdf_url": pdf_url,
                                "document_title": link_text[:100],
                                "has_markdown": PDF_SUPPORT and md_content != "",
                            })
                        else:
                            print(f"   ‚ö†Ô∏è  –ù–µ PDF (Content-Type: {content_type})")
                            download_errors.append({
                                "case_number": case["case_number"],
                                "error": f"wrong_content_type: {content_type}"
                            })
                    else:
                        print(f"   ‚ùå HTTP {response.status_code}")
                        download_errors.append({
                            "case_number": case["case_number"],
                            "error": f"http_{response.status_code}"
                        })

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                download_errors.append({
                    "case_number": case["case_number"],
                    "error": str(e)
                })
                continue

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(1)

        print()
        print(f"‚úÖ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {downloaded_count}/{NUM_PDFS} PDF")
        if PDF_SUPPORT:
            print(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {converted_count}/{downloaded_count} MD")
        print()

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        pdf_metadata_file = data_dir / "january_2024_pdf_metadata.json"
        pdf_metadata_file.write_text(
            json.dumps(pdf_metadata, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

        # ============================================================
        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        # ============================================================

        print("=" * 80)
        print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 80)
        print()

        stats = {
            "parsing": {
                "total_cases": len(all_cases),
                "pages_parsed": total_pages,
                "parsing_time_sec": parsing_time,
            },
            "downloading": {
                "pdfs_requested": NUM_PDFS,
                "pdfs_downloaded": downloaded_count,
                "pdfs_converted_to_md": converted_count if PDF_SUPPORT else 0,
                "success_rate": downloaded_count / NUM_PDFS if NUM_PDFS > 0 else 0,
                "errors": len(download_errors),
            },
            "storage": {
                "pdf_dir": str(pdfs_dir),
                "md_dir": str(mds_dir) if PDF_SUPPORT else None,
                "total_pdf_size_mb": sum(m["pdf_size"] for m in pdf_metadata) / (1024 * 1024),
            }
        }

        stats_file = data_dir / "january_2024_stats.json"
        stats_file.write_text(
            json.dumps(stats, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

        print(f"üìä –î–µ–ª —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(all_cases)}")
        print(f"üìä PDF —Å–∫–∞—á–∞–Ω–æ: {downloaded_count}/{NUM_PDFS}")
        if PDF_SUPPORT:
            print(f"üìä MD –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {converted_count}/{downloaded_count}")
        print(f"üìä –í—Ä–µ–º—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {parsing_time:.1f} —Å–µ–∫")
        print()

        print("üíæ –§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        print(f"   - {cases_file}")
        print(f"   - {pdf_metadata_file}")
        print(f"   - {stats_file}")
        print(f"   - {pdfs_dir}/ ({downloaded_count} PDF)")
        if PDF_SUPPORT:
            print(f"   - {mds_dir}/ ({converted_count} MD)")
        print()

        print("=" * 80)
        print("üéâ PIPELINE –ó–ê–í–ï–†–®–ï–ù!")
        print("=" * 80)


if __name__ == "__main__":
    asyncio.run(parse_and_download())
