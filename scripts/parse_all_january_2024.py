#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —è–Ω–≤–∞—Ä—è 2024: –≤—Å–µ 40 —Å—Ç—Ä–∞–Ω–∏—Ü + —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ 100 PDF.

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤:
- data/january_2024_cases.json - –≤—Å–µ —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–µ–ª–∞
- data/january_2024_pdfs/ - 100 PDF –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
- data/january_2024_stats.json - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
"""

import asyncio
import json
import random
from pathlib import Path
from datetime import datetime
from typing import Any

import httpx
from structlog import get_logger

from src.scraper.playwright_scraper import PlaywrightScraper

logger = get_logger(__name__)


async def parse_all_january_2024():
    """–°–ø–∞—Ä—Å–∏—Ç—å –≤—Å–µ 40 —Å—Ç—Ä–∞–Ω–∏—Ü —è–Ω–≤–∞—Ä—è 2024 –∏ —Å–∫–∞—á–∞—Ç—å 100 PDF."""

    print("=" * 80)
    print("üöÄ –ü–û–õ–ù–û–ú–ê–°–®–¢–ê–ë–ù–´–ô –ü–ê–†–°–ò–ù–ì –Ø–ù–í–ê–†–Ø 2024")
    print("=" * 80)
    print()

    # –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)

    pdfs_dir = data_dir / "january_2024_pdfs"
    pdfs_dir.mkdir(exist_ok=True)

    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Chrome —á–µ—Ä–µ–∑ CDP
    async with PlaywrightScraper(
        use_cdp=True,
        cdp_url="http://localhost:9222",
    ) as scraper:
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Chrome —á–µ—Ä–µ–∑ CDP\n")

        # ============================================================
        # –®–ê–ì 1: –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö 40 —Å—Ç—Ä–∞–Ω–∏—Ü
        # ============================================================

        print("=" * 80)
        print("–®–ê–ì 1: –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö 40 —Å—Ç—Ä–∞–Ω–∏—Ü (—è–Ω–≤–∞—Ä—å 2024)")
        print("=" * 80)
        print()

        # –û—Ç–∫—Ä—ã—Ç—å –ö–ê–î –ê—Ä–±–∏—Ç—Ä
        await scraper.page.goto("https://kad.arbitr.ru", wait_until="networkidle")
        await asyncio.sleep(2)

        # –ó–∞–∫—Ä—ã—Ç—å popup –µ—Å–ª–∏ –µ—Å—Ç—å
        try:
            await scraper.page.keyboard.press("Escape")
            await asyncio.sleep(1)
        except Exception:
            pass

        # –ó–∞–ø–æ–ª–Ω–∏—Ç—å —Ñ–æ—Ä–º—É –ø–æ–∏—Å–∫–∞ (—è–Ω–≤–∞—Ä—å 2024)
        date_inputs = await scraper.page.query_selector_all('input[placeholder="–¥–¥.–º–º.–≥–≥–≥–≥"]')
        if len(date_inputs) >= 2:
            await date_inputs[0].click()
            await asyncio.sleep(0.2)
            await date_inputs[0].fill("01.01.2024")
            await asyncio.sleep(0.5)

            await date_inputs[1].click()
            await asyncio.sleep(0.2)
            await date_inputs[1].fill("31.01.2024")
            await asyncio.sleep(0.5)

        await scraper.page.click("body")
        await asyncio.sleep(0.5)

        # –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–æ—Ä–º—É
        await scraper.page.click("#b-form-submit")
        await asyncio.sleep(5)

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        total_pages_input = await scraper.page.query_selector("input#documentsPagesCount")
        if not total_pages_input:
            print("‚ùå –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return

        total_pages_str = await total_pages_input.get_attribute("value")
        total_pages = int(total_pages_str) if total_pages_str else 0

        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
        print(f"üìÑ –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å: –í–°–ï {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
        print()

        # –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        all_cases = []
        start_time = datetime.now()

        for page_num in range(1, total_pages + 1):
            print(f"üìñ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}/{total_pages}...")

            # –ü–æ–ª—É—á–∏—Ç—å HTML —Ç–∞–±–ª–∏—Ü—ã
            table = await scraper.page.query_selector("table.b-cases")
            if not table:
                print(f"   ‚ö†Ô∏è  –¢–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
                continue

            table_html = await table.inner_html()

            # –ü–∞—Ä—Å–∏–Ω–≥
            try:
                cases = scraper._parse_table_html(table_html)
                all_cases.extend(cases)
                print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ –¥–µ–ª: {len(cases)} (–≤—Å–µ–≥–æ: {len(all_cases)})")

                # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥—ã–µ 10 —Å—Ç—Ä–∞–Ω–∏—Ü
                if page_num % 10 == 0:
                    temp_file = data_dir / f"january_2024_cases_page_{page_num}.json"
                    temp_file.write_text(
                        json.dumps(all_cases, ensure_ascii=False, indent=2),
                        encoding="utf-8"
                    )
                    print(f"   üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {temp_file}")

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
                continue

            # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            if page_num < total_pages:
                try:
                    link = await scraper.page.query_selector(f'a[href="#page{page_num + 1}"]')
                    if link:
                        await link.click()
                        await asyncio.sleep(5)  # –ñ–¥–µ–º –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫—É —Ç–∞–±–ª–∏—Ü—ã
                    else:
                        print(f"   ‚ö†Ô∏è  –°—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num + 1} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                        break
                except Exception as e:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num + 1}: {e}")
                    break

        parsing_time = (datetime.now() - start_time).total_seconds()

        print()
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(all_cases)} –¥–µ–ª –∑–∞ {parsing_time:.1f} —Å–µ–∫")
        print()

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ –¥–µ–ª–∞
        cases_file = data_dir / "january_2024_cases.json"
        cases_file.write_text(
            json.dumps(all_cases, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {cases_file}")
        print()

        # ============================================================
        # –®–ê–ì 2: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ 100 PDF –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        # ============================================================

        print("=" * 80)
        print("–®–ê–ì 2: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ 100 PDF –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        print("=" * 80)
        print()

        # –í—ã–±—Ä–∞—Ç—å 100 —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–ª
        NUM_PDFS = min(100, len(all_cases))
        selected_cases = random.sample(all_cases, NUM_PDFS)

        print(f"üìã –í—ã–±—Ä–∞–Ω–æ –¥–µ–ª –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF: {NUM_PDFS}")
        print()

        downloaded_count = 0
        download_errors = []
        pdf_metadata = []

        for i, case in enumerate(selected_cases, 1):
            print(f"üìÑ –î–µ–ª–æ {i}/{NUM_PDFS}: {case['case_number']}")

            try:
                # –û—Ç–∫—Ä—ã—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–µ–ª–∞
                case_url = f"https://kad.arbitr.ru{case['url']}"
                await scraper.page.goto(case_url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(2)

                print(f"   ‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–µ–ª–∞ –æ—Ç–∫—Ä—ã—Ç–∞")

                # –ü–æ–∏—Å–∫ –ø—Ä—è–º—ã—Ö PDF —Å—Å—ã–ª–æ–∫
                doc_links = await scraper.page.query_selector_all('a[href$=".pdf"]')

                if not doc_links:
                    print(f"   ‚ö†Ô∏è  PDF —Å—Å—ã–ª–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    download_errors.append({
                        "case_number": case["case_number"],
                        "error": "no_pdf_links"
                    })
                    continue

                print(f"   –ù–∞–π–¥–µ–Ω–æ PDF —Å—Å—ã–ª–æ–∫: {len(doc_links)}")

                # –ü–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Å—ã–ª–∫—É
                first_link = doc_links[0]
                link_text = await first_link.inner_text()
                pdf_url = await first_link.get_attribute("href")

                print(f"   –î–æ–∫—É–º–µ–Ω—Ç: {link_text[:50]}")

                # –ò–∑–≤–ª–µ—á—å –∏–º—è —Ñ–∞–π–ª–∞
                pdf_filename = pdf_url.split("/")[-1] if pdf_url else "document.pdf"

                # –°–∫–∞—á–∞—Ç—å —á–µ—Ä–µ–∑ HTTP —Å cookies
                try:
                    # –ü–æ–ª—É—á–∏—Ç—å cookies –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞
                    cookies = await scraper.page.context.cookies()
                    cookie_dict = {cookie['name']: cookie['value'] for cookie in cookies}

                    # –°–∫–∞—á–∞—Ç—å PDF
                    async with httpx.AsyncClient(
                        cookies=cookie_dict,
                        timeout=30.0,
                        follow_redirects=True
                    ) as client:
                        response = await client.get(pdf_url)

                        if response.status_code == 200:
                            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —ç—Ç–æ PDF
                            content_type = response.headers.get('content-type', '')

                            if 'pdf' in content_type.lower() or pdf_url.endswith('.pdf'):
                                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª
                                filename = f"{case['case_number'].replace('/', '_')}_{pdf_filename}"
                                filepath = pdfs_dir / filename

                                filepath.write_bytes(response.content)

                                print(f"   ‚úÖ –°–∫–∞—á–∞–Ω: {filename} ({len(response.content)} bytes)")
                                downloaded_count += 1

                                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                                pdf_metadata.append({
                                    "case_number": case["case_number"],
                                    "case_date": case["case_date"],
                                    "judge": case.get("judge"),
                                    "court": case.get("court"),
                                    "pdf_filename": filename,
                                    "pdf_size": len(response.content),
                                    "pdf_url": pdf_url,
                                    "document_title": link_text[:100],
                                })
                            else:
                                print(f"   ‚ö†Ô∏è  –ù–µ PDF —Ñ–∞–π–ª (Content-Type: {content_type})")
                                download_errors.append({
                                    "case_number": case["case_number"],
                                    "error": f"wrong_content_type: {content_type}"
                                })
                        else:
                            print(f"   ‚ùå HTTP –æ—à–∏–±–∫–∞: {response.status_code}")
                            download_errors.append({
                                "case_number": case["case_number"],
                                "error": f"http_{response.status_code}"
                            })

                except Exception as download_error:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {download_error}")
                    download_errors.append({
                        "case_number": case["case_number"],
                        "error": str(download_error)
                    })
                    continue

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                download_errors.append({
                    "case_number": case["case_number"],
                    "error": str(e)
                })
                continue

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(1)

        print()
        print(f"‚úÖ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {downloaded_count}/{NUM_PDFS} PDF")
        print()

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ PDF
        pdf_metadata_file = data_dir / "january_2024_pdf_metadata.json"
        pdf_metadata_file.write_text(
            json.dumps(pdf_metadata, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

        # ============================================================
        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        # ============================================================

        print("=" * 80)
        print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 80)
        print()

        # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        case_types = {}
        courts = {}
        judges = {}

        for case in all_cases:
            # –ü–æ–¥—Å—á–µ—Ç —Ç–∏–ø–æ–≤ –¥–µ–ª
            case_type = case.get("case_type", "unknown")
            case_types[case_type] = case_types.get(case_type, 0) + 1

            # –ü–æ–¥—Å—á–µ—Ç —Å—É–¥–æ–≤
            court = case.get("court", "unknown")
            courts[court] = courts.get(court, 0) + 1

            # –ü–æ–¥—Å—á–µ—Ç —Å—É–¥–µ–π
            judge = case.get("judge", "unknown")
            judges[judge] = judges.get(judge, 0) + 1

        stats = {
            "parsing": {
                "total_cases": len(all_cases),
                "pages_parsed": total_pages,
                "parsing_time_sec": parsing_time,
                "avg_time_per_page": parsing_time / total_pages if total_pages > 0 else 0,
            },
            "downloading": {
                "pdfs_requested": NUM_PDFS,
                "pdfs_downloaded": downloaded_count,
                "success_rate": downloaded_count / NUM_PDFS if NUM_PDFS > 0 else 0,
                "errors": len(download_errors),
            },
            "categories": {
                "case_types": case_types,
                "courts": dict(sorted(courts.items(), key=lambda x: x[1], reverse=True)[:20]),
                "judges": dict(sorted(judges.items(), key=lambda x: x[1], reverse=True)[:20]),
            },
            "errors": download_errors,
        }

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats_file = data_dir / "january_2024_stats.json"
        stats_file.write_text(
            json.dumps(stats, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

        print(f"üìä –í—Å–µ–≥–æ –¥–µ–ª —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(all_cases)}")
        print(f"üìä PDF —Å–∫–∞—á–∞–Ω–æ: {downloaded_count}/{NUM_PDFS}")
        print(f"üìä –°—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_pages}")
        print(f"üìä –í—Ä–µ–º—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {parsing_time:.1f} —Å–µ–∫ ({parsing_time/60:.1f} –º–∏–Ω)")
        print()

        print("üìà –¢–æ–ø-5 —Ç–∏–ø–æ–≤ –¥–µ–ª:")
        for case_type, count in sorted(case_types.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"   {case_type}: {count}")
        print()

        print("üìà –¢–æ–ø-5 —Å—É–¥–æ–≤:")
        for court, count in sorted(courts.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"   {court}: {count}")
        print()

        print("üíæ –§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        print(f"   - {cases_file}")
        print(f"   - {pdf_metadata_file}")
        print(f"   - {stats_file}")
        print(f"   - {pdfs_dir}/ ({downloaded_count} —Ñ–∞–π–ª–æ–≤)")
        print()

        print("=" * 80)
        print("üéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        print("=" * 80)


if __name__ == "__main__":
    asyncio.run(parse_all_january_2024())
