#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ–ª–∞ –ö–ê–î –ê—Ä–±–∏—Ç—Ä.

–ó–∞–¥–∞—á–∞: –Ω–∞–π—Ç–∏ –≤–∫–ª–∞–¥–∫–∏ –∏–Ω—Å—Ç–∞–Ω—Ü–∏–π –∏ –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –º–µ–∂–¥—É –Ω–∏–º–∏.
"""

import asyncio
import json
from pathlib import Path

from src.scraper.playwright_scraper import PlaywrightScraper


async def analyze_case_page():
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–¥–Ω–æ–≥–æ –¥–µ–ª–∞."""

    print("=" * 80)
    print("–ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –°–¢–†–ê–ù–ò–¶–´ –î–ï–õ–ê")
    print("=" * 80)
    print()

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–µ–ª–∞
    cases_file = Path("data/january_2024_cases.json")
    if not cases_file.exists():
        print("‚ùå –§–∞–π–ª data/january_2024_cases.json –Ω–µ –Ω–∞–π–¥–µ–Ω")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞ –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return

    with open(cases_file, encoding="utf-8") as f:
        all_cases = json.load(f)

    if not all_cases:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –¥–µ–ª–∞—Ö")
        return

    # –í–∑—è—Ç—å –ø–µ—Ä–≤–æ–µ –¥–µ–ª–æ
    case = all_cases[0]
    print(f"üìã –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ–ª–æ: {case['case_number']}")
    print(f"   –°—É–¥: {case.get('court', 'N/A')}")
    print()

    # –û—á–∏—Å—Ç–∏—Ç—å –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å URL
    case_url = case['url']
    case_url = case_url.replace('https//kad.arbitr.ru', '')
    case_url = case_url.replace('http//kad.arbitr.ru', '')
    case_url = case_url.replace('//kad.arbitr.ru', '')
    case_url = case_url.replace('https://kad.arbitr.ru', '')
    case_url = case_url.replace('http://kad.arbitr.ru', '')

    if not case_url.startswith('/'):
        case_url = '/' + case_url

    case_url = f"https://kad.arbitr.ru{case_url}"

    print(f"üîó URL: {case_url}")
    print()

    async with PlaywrightScraper(use_cdp=True, cdp_url="http://localhost:9222") as scraper:
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Chrome\n")

        # –û—Ç–∫—Ä—ã—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–µ–ª–∞
        print("‚è≥ –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–µ–ª–∞...")
        await scraper.page.goto(case_url, wait_until="networkidle", timeout=30000)
        await asyncio.sleep(3)
        print("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")

        # ============================================================
        # 1. –ü–û–ò–°–ö –í–ö–õ–ê–î–û–ö / –¢–ê–ë–û–í
        # ============================================================

        print("=" * 80)
        print("1. –ü–û–ò–°–ö –í–ö–õ–ê–î–û–ö –ò–ù–°–¢–ê–ù–¶–ò–ô")
        print("=" * 80)
        print()

        # –ò—Å–∫–∞—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º –ø—Ä–æ –∏–Ω—Å—Ç–∞–Ω—Ü–∏–∏
        keywords = [
            "–ü–µ—Ä–≤–∞—è –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è",
            "–ê–ø–µ–ª–ª—è—Ü–∏—è",
            "–ö–∞—Å—Å–∞—Ü–∏—è",
            "–ø–µ—Ä–≤–∞—è",
            "–∞–ø–µ–ª–ª—è—Ü",
            "–∫–∞—Å—Å–∞—Ü",
        ]

        found_tabs = []

        for keyword in keywords:
            # –ò—Å–∫–∞—Ç—å –∫–Ω–æ–ø–∫–∏
            buttons = await scraper.page.query_selector_all(f"button:has-text('{keyword}')")
            for btn in buttons[:3]:  # –ü–µ—Ä–≤—ã–µ 3
                text = await btn.inner_text()
                visible = await btn.is_visible()
                found_tabs.append({
                    "type": "button",
                    "text": text.strip(),
                    "visible": visible,
                })

            # –ò—Å–∫–∞—Ç—å —Å—Å—ã–ª–∫–∏
            links = await scraper.page.query_selector_all(f"a:has-text('{keyword}')")
            for link in links[:3]:
                text = await link.inner_text()
                visible = await link.is_visible()
                href = await link.get_attribute("href")
                found_tabs.append({
                    "type": "link",
                    "text": text.strip(),
                    "visible": visible,
                    "href": href,
                })

            # –ò—Å–∫–∞—Ç—å div/span —Å –∫–ª–∞—Å—Å–∞–º–∏ tab, instance
            divs = await scraper.page.query_selector_all(
                f"div:has-text('{keyword}'), span:has-text('{keyword}')"
            )
            for div in divs[:3]:
                text = await div.inner_text()
                visible = await div.is_visible()
                class_name = await div.get_attribute("class")
                found_tabs.append({
                    "type": "div/span",
                    "text": text.strip()[:50],
                    "visible": visible,
                    "class": class_name,
                })

        if found_tabs:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –∏–Ω—Å—Ç–∞–Ω—Ü–∏–π: {len(found_tabs)}\n")
            for i, tab in enumerate(found_tabs[:10], 1):  # –ü–µ—Ä–≤—ã–µ 10
                print(f"{i}. {tab}")
        else:
            print("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ —è–≤–Ω—ã—Ö –≤–∫–ª–∞–¥–æ–∫ —Å —Ç–µ–∫—Å—Ç–æ–º –ø—Ä–æ –∏–Ω—Å—Ç–∞–Ω—Ü–∏–∏\n")

        # ============================================================
        # 2. –ü–û–ò–°–ö –¢–ê–ë–õ–ò–¶ –° –î–û–ö–£–ú–ï–ù–¢–ê–ú–ò
        # ============================================================

        print("\n")
        print("=" * 80)
        print("2. –ü–û–ò–°–ö –¢–ê–ë–õ–ò–¶ –° –î–û–ö–£–ú–ï–ù–¢–ê–ú–ò")
        print("=" * 80)
        print()

        # –ò—Å–∫–∞—Ç—å —Ç–∞–±–ª–∏—Ü—ã
        all_tables = await scraper.page.query_selector_all("table")
        print(f"üìä –í—Å–µ–≥–æ —Ç–∞–±–ª–∏—Ü –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(all_tables)}\n")

        for i, table in enumerate(all_tables, 1):
            # –ü–æ–ª—É—á–∏—Ç—å ID –∏ –∫–ª–∞—Å—Å
            table_id = await table.get_attribute("id")
            table_class = await table.get_attribute("class")

            print(f"–¢–∞–±–ª–∏—Ü–∞ {i}:")
            print(f"   ID: {table_id or 'N/A'}")
            print(f"   Class: {table_class or 'N/A'}")

            # –ü–æ—Å—á–∏—Ç–∞—Ç—å —Å—Ç—Ä–æ–∫–∏
            rows = await table.query_selector_all("tr")
            print(f"   –°—Ç—Ä–æ–∫: {len(rows)}")

            # –ù–∞–π—Ç–∏ PDF —Å—Å—ã–ª–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ
            pdf_links = await table.query_selector_all('a[href$=".pdf"]')
            print(f"   PDF —Å—Å—ã–ª–æ–∫: {len(pdf_links)}")

            if pdf_links:
                print(f"   ‚úÖ –≠—Ç–æ —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏!")
                # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 3 —Å—Å—ã–ª–∫–∏
                for j, link in enumerate(pdf_links[:3], 1):
                    text = await link.inner_text()
                    href = await link.get_attribute("href")
                    print(f"      {j}. {text.strip()[:60]}")
                    print(f"         URL: {href[:80]}")

            print()

        # ============================================================
        # 3. –ü–û–ò–°–ö –í–°–ï–• PDF –°–°–´–õ–û–ö
        # ============================================================

        print("=" * 80)
        print("3. –í–°–ï PDF –°–°–´–õ–ö–ò –ù–ê –°–¢–†–ê–ù–ò–¶–ï")
        print("=" * 80)
        print()

        all_pdf_links = await scraper.page.query_selector_all('a[href$=".pdf"]')
        print(f"üìÑ –í—Å–µ–≥–æ PDF —Å—Å—ã–ª–æ–∫: {len(all_pdf_links)}\n")

        if all_pdf_links:
            print("–ü–µ—Ä–≤—ã–µ 10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
            for i, link in enumerate(all_pdf_links[:10], 1):
                text = await link.inner_text()
                href = await link.get_attribute("href")
                visible = await link.is_visible()
                print(f"{i}. [{('‚úì' if visible else '‚úó')}] {text.strip()[:60]}")
                print(f"   {href}")
                print()

        # ============================================================
        # 4. HTML –°–¢–†–£–ö–¢–£–†–ê
        # ============================================================

        print("=" * 80)
        print("4. HTML –°–¢–†–£–ö–¢–£–†–ê (–¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)")
        print("=" * 80)
        print()

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        html_content = await scraper.page.content()
        html_file = Path("data/case_page_structure.html")
        html_file.write_text(html_content, encoding="utf-8")

        print(f"üíæ HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {html_file}")
        print(f"   –†–∞–∑–º–µ—Ä: {len(html_content)} –±–∞–π—Ç")
        print()

        print("=" * 80)
        print("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
        print("=" * 80)
        print()
        print("üìã –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("   1. –ò–∑—É—á–∏—Ç–µ –≤—ã–≤–æ–¥ –≤—ã—à–µ")
        print("   2. –û—Ç–∫—Ä–æ–π—Ç–µ data/case_page_structure.html –≤ –±—Ä–∞—É–∑–µ—Ä–µ")
        print("   3. –ù–∞–π–¥–∏—Ç–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –≤–∫–ª–∞–¥–æ–∫ –∏–Ω—Å—Ç–∞–Ω—Ü–∏–π")
        print("   4. –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, –∫–∞–∫ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –º–µ–∂–¥—É –≤–∫–ª–∞–¥–∫–∞–º–∏")


if __name__ == "__main__":
    asyncio.run(analyze_case_page())
