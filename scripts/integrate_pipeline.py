#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π pipeline - —Å–≤—è–∑—ã–≤–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π workflow —Å –Ω–æ–≤—ã–º–∏ –º–æ–¥—É–ª—è–º–∏.

WORKFLOW:
1. –ü–∞—Ä—Å–∏–Ω–≥: parse_january_by_day_and_court.py ‚Üí JSON
2. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ: download_all_electronic_case_docs.py ‚Üí PDFs
3. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: converter ‚Üí Markdown
4. –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: sqlite_manager ‚Üí metadata storage

Usage:
    python scripts/integrate_pipeline.py \
        --json data/january_2024_cases.json \
        --db data/kad_2024.db \
        --downloads downloads/ \
        --documents documents/
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
import shutil

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.scraper.playwright_scraper import PlaywrightScraper
from src.database import SQLiteManager
from src.converter import convert_pdf_to_md, batch_convert
import httpx


async def download_case_documents(scraper, case: dict, downloads_dir: Path):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–µ–ª–∞ (–ª–æ–≥–∏–∫–∞ –∏–∑ download_all_electronic_case_docs.py).

    Args:
        scraper: PlaywrightScraper instance
        case: Case data dict with 'case_number' and 'url'
        downloads_dir: Directory for downloads

    Returns:
        List of downloaded PDF paths
    """
    case_number = case['case_number']
    case_url = case['url']

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
    case_url = case_url.replace('https//kad.arbitr.ru', '').replace('http//kad.arbitr.ru', '').replace('//kad.arbitr.ru', '').replace('https://kad.arbitr.ru', '').replace('http://kad.arbitr.ru', '').replace('https:/', '').replace('http:/', '')
    if not case_url.startswith('/'):
        case_url = '/' + case_url
    case_url = f"https://kad.arbitr.ru{case_url}"

    print(f"\nüìã –î–µ–ª–æ: {case_number}")
    print(f"üîó URL: {case_url}")

    # –°–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    case_folder = downloads_dir / case_number.replace('/', '_')
    case_folder.mkdir(parents=True, exist_ok=True)

    # –û—Ç–∫—Ä—ã—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–µ–ª–∞
    await scraper.page.goto(case_url, wait_until="networkidle", timeout=30000)
    await asyncio.sleep(2)

    # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –≤–∫–ª–∞–¥–∫—É "–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ"
    electronic_tab = await scraper.page.query_selector(".js-case-chrono-button--ed")

    if not electronic_tab:
        print("‚ùå –í–∫–ª–∞–¥–∫–∞ '–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return []

    await electronic_tab.click()
    await asyncio.sleep(3)

    # –ü–æ–ª—É—á–∏—Ç—å cookies –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    cookies = await scraper.page.context.cookies()
    cookie_dict = {cookie['name']: cookie['value'] for cookie in cookies}

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    page_num = 1
    downloaded_files = []
    downloaded_urls = set()

    while True:
        # –ù–∞–π—Ç–∏ –≤—Å–µ PDF –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        pdf_links = await scraper.page.query_selector_all('a[href$=".pdf"]')

        if not pdf_links:
            break

        # –°–∫–∞—á–∞—Ç—å –∫–∞–∂–¥—ã–π PDF
        for i, link in enumerate(pdf_links, 1):
            try:
                href = await link.get_attribute("href")

                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –µ—Å–ª–∏ —É–∂–µ —Å–∫–∞—á–∏–≤–∞–ª–∏
                if href in downloaded_urls:
                    continue

                downloaded_urls.add(href)

                # –°–∫–∞—á–∞—Ç—å PDF —á–µ—Ä–µ–∑ HTTP —Å retry
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        async with httpx.AsyncClient(
                            cookies=cookie_dict,
                            timeout=30.0,
                            follow_redirects=True
                        ) as client:
                            response = await client.get(href)

                            if response.status_code == 200:
                                content_type = response.headers.get('content-type', '')

                                if 'pdf' in content_type.lower() or href.endswith('.pdf'):
                                    # –ò–∑–≤–ª–µ—á—å –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
                                    filename = href.split("/")[-1]
                                    if not filename.endswith('.pdf'):
                                        filename += '.pdf'

                                    # –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–º–µ—Ä –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                                    filepath = case_folder / f"{len(downloaded_files) + 1:03d}_{filename}"

                                    filepath.write_bytes(response.content)
                                    downloaded_files.append(filepath)

                                    print(f"   ‚úÖ {len(response.content)//1024} KB ‚Üí {filepath.name}")
                                    break

                            else:
                                if attempt < max_retries - 1:
                                    await asyncio.sleep(2)

                    except Exception as e:
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2)
                        else:
                            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {str(e)[:60]}")

            except Exception as e:
                print(f"   ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)[:60]}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        next_button = await scraper.page.query_selector(".js-chrono-pagination-pager-item--arrow.next")

        if not next_button:
            next_button = await scraper.page.query_selector(".js-card-list_paginator-item.next")

        if not next_button:
            break

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –∫–Ω–æ–ø–∫–∞ –∞–∫—Ç–∏–≤–Ω–∞
        try:
            is_disabled = await next_button.evaluate("el => el.classList.contains('disabled') || el.hasAttribute('disabled')")
            if is_disabled:
                break
        except:
            pass

        # –ö–ª–∏–∫–Ω—É—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        try:
            await next_button.click()
            await asyncio.sleep(3)
            page_num += 1
        except:
            break

    print(f"‚úÖ –°–∫–∞—á–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(downloaded_files)}")
    return downloaded_files


def convert_pdfs_to_markdown(pdf_files: list, documents_dir: Path, case_number: str):
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PDF ‚Üí Markdown.

    Args:
        pdf_files: List of PDF file paths
        documents_dir: Base documents directory
        case_number: Case number

    Returns:
        List of created MD file paths
    """
    if not pdf_files:
        return []

    print(f"\nüîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è {len(pdf_files)} PDF ‚Üí Markdown...")

    # –°–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É –¥–ª—è MD —Ñ–∞–π–ª–æ–≤
    # –ò–∑–≤–ª–µ—á—å –≥–æ–¥ –∏–∑ –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ê40-12345-2024 ‚Üí 2024)
    parts = case_number.split('-')
    year = parts[-1] if len(parts) >= 3 else "unknown"

    case_md_dir = documents_dir / year / case_number.replace('/', '_')
    case_md_dir.mkdir(parents=True, exist_ok=True)

    md_files = []
    success_count = 0
    failed_count = 0

    for pdf_path in pdf_files:
        try:
            # –°–æ–∑–¥–∞—Ç—å MD –ø—É—Ç—å
            md_filename = pdf_path.stem + '.md'
            md_path = case_md_dir / md_filename

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å
            success = convert_pdf_to_md(str(pdf_path), str(md_path))

            if success:
                md_files.append(md_path)
                success_count += 1
                print(f"   ‚úÖ {pdf_path.name} ‚Üí {md_filename}")
            else:
                failed_count += 1
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {pdf_path.name}")

        except Exception as e:
            failed_count += 1
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {pdf_path.name} - {str(e)[:60]}")

    print(f"\nüìä –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {success_count} —É—Å–ø–µ—à–Ω–æ, {failed_count} –æ—à–∏–±–æ–∫")
    return md_files


def store_in_database(db: SQLiteManager, case: dict, md_files: list):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–µ–ª–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î.

    Args:
        db: SQLiteManager instance
        case: Case data dict
        md_files: List of Markdown file paths
    """
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î...")

    # –í—Å—Ç–∞–≤–∏—Ç—å –¥–µ–ª–æ (–µ—Å–ª–∏ –µ—â–µ –Ω–µ—Ç)
    if not db.case_exists(case['case_number']):
        db.insert_case({
            'case_number': case['case_number'],
            'court': case.get('court', ''),
            'registration_date': case.get('case_date', ''),
            'status': '',
            'parties': ''
        })
        print(f"   ‚úÖ –î–µ–ª–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {case['case_number']}")
    else:
        print(f"   ‚ÑπÔ∏è  –î–µ–ª–æ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {case['case_number']}")

    # –í—Å—Ç–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
    for md_path in md_files:
        doc_data = {
            'case_number': case['case_number'],
            'doc_type': md_path.stem,  # –ò–º—è —Ñ–∞–π–ª–∞ –∫–∞–∫ —Ç–∏–ø
            'instance': '',
            'is_final': False,
            'pdf_url': '',
            'md_path': str(md_path),
            'file_size': md_path.stat().st_size if md_path.exists() else 0
        }

        db.insert_document(doc_data)

    print(f"   ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {len(md_files)}")


async def process_single_case(scraper, db: SQLiteManager, case: dict, downloads_dir: Path, documents_dir: Path, cleanup_pdfs: bool = True):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω–æ –¥–µ–ª–æ: —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ ‚Üí –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è ‚Üí –ë–î.

    Args:
        scraper: PlaywrightScraper instance
        db: SQLiteManager instance
        case: Case data dict
        downloads_dir: Directory for PDF downloads
        documents_dir: Directory for MD documents
        cleanup_pdfs: Delete PDFs after conversion (default: True)
    """
    case_number = case['case_number']

    print("\n" + "=" * 80)
    print(f"–û–ë–†–ê–ë–û–¢–ö–ê –î–ï–õ–ê: {case_number}")
    print("=" * 80)

    try:
        # 1. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF
        pdf_files = await download_case_documents(scraper, case, downloads_dir)

        if not pdf_files:
            print(f"‚ö†Ô∏è  –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è")
            return

        # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PDF ‚Üí MD
        md_files = convert_pdfs_to_markdown(pdf_files, documents_dir, case_number)

        # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        store_in_database(db, case, md_files)

        # 4. –û—á–∏—Å—Ç–∫–∞ PDF (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if cleanup_pdfs and pdf_files:
            case_folder = pdf_files[0].parent
            try:
                shutil.rmtree(case_folder)
                print(f"\nüóëÔ∏è  PDF —É–¥–∞–ª–µ–Ω—ã (—ç–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞)")
            except:
                pass

        print(f"\n‚úÖ –î–ï–õ–û –û–ë–†–ê–ë–û–¢–ê–ù–û: {case_number}")

    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê –ü–†–ò –û–ë–†–ê–ë–û–¢–ö–ï –î–ï–õ–ê {case_number}: {str(e)}")
        import traceback
        traceback.print_exc()


async def process_multiple_cases(json_path: str, db_path: str, downloads_dir: str = "downloads", documents_dir: str = "documents", start_index: int = 0, max_cases: int = None, cdp_url: str = "http://localhost:9222"):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–µ–ª –∏–∑ JSON —Ñ–∞–π–ª–∞.

    Args:
        json_path: Path to JSON file with cases
        db_path: Path to SQLite database
        downloads_dir: Directory for PDF downloads
        documents_dir: Directory for MD documents
        start_index: Start from this case index (for resuming)
        max_cases: Maximum number of cases to process (None = all)
        cdp_url: Chrome CDP URL (default: http://localhost:9222)
    """
    print("=" * 80)
    print("üöÄ –ò–ù–¢–ï–ì–†–ê–¶–ò–û–ù–ù–´–ô PIPELINE")
    print("=" * 80)
    print()

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–µ–ª–∞ –∏–∑ JSON
    json_file = Path(json_path)
    if not json_file.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {json_path}")
        return

    with open(json_file, encoding='utf-8') as f:
        all_cases = json.load(f)

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞
    cases_to_process = all_cases[start_index:]
    if max_cases:
        cases_to_process = cases_to_process[:max_cases]

    print(f"üìÇ JSON —Ñ–∞–π–ª: {json_path}")
    print(f"üìä –í—Å–µ–≥–æ –¥–µ–ª –≤ —Ñ–∞–π–ª–µ: {len(all_cases)}")
    print(f"üìä –î–µ–ª –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(cases_to_process)} (–∏–Ω–¥–µ–∫—Å {start_index} - {start_index + len(cases_to_process) - 1})")
    print()

    # –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    downloads_path = Path(downloads_dir)
    documents_path = Path(documents_dir)
    downloads_path.mkdir(exist_ok=True)
    documents_path.mkdir(exist_ok=True)

    # –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –ë–î
    db = SQLiteManager(db_path)
    print(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {db_path}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = db.get_stats()
    print(f"   –î–µ–ª –≤ –ë–î: {stats['total_cases']}")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î: {stats['total_documents']}")
    print()

    # –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Chrome —á–µ—Ä–µ–∑ CDP
    print(f"üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Chrome (CDP: {cdp_url})...")
    async with PlaywrightScraper(use_cdp=True, cdp_url=cdp_url) as scraper:
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Chrome")
        print()

        start_time = datetime.now()

        # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∂–¥–æ–µ –¥–µ–ª–æ
        for idx, case in enumerate(cases_to_process, start=start_index + 1):
            print(f"\n[{idx}/{len(all_cases)}] ", end="")

            try:
                await process_single_case(
                    scraper,
                    db,
                    case,
                    downloads_path,
                    documents_path,
                    cleanup_pdfs=True
                )

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –¥–µ–ª–∞–º–∏
                if idx < len(cases_to_process):
                    await asyncio.sleep(3.0)

            except Exception as e:
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
                continue

        elapsed = (datetime.now() - start_time).total_seconds()

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "=" * 80)
        print("üéâ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
        print("=" * 80)
        print()

        final_stats = db.get_stats()
        print(f"üìä –î–µ–ª –≤ –ë–î: {final_stats['total_cases']}")
        print(f"üìä –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î: {final_stats['total_documents']}")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {elapsed/60:.1f} –º–∏–Ω—É—Ç")
        print()


def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(description="–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π pipeline –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–µ–ª")
    parser.add_argument('--json', required=True, help='Path to JSON file with cases')
    parser.add_argument('--db', required=True, help='Path to SQLite database')
    parser.add_argument('--downloads', default='downloads', help='Directory for PDF downloads')
    parser.add_argument('--documents', default='documents', help='Directory for MD documents')
    parser.add_argument('--start-index', type=int, default=0, help='Start from case index (for resuming)')
    parser.add_argument('--max-cases', type=int, help='Maximum number of cases to process')
    parser.add_argument('--cdp-url', default='http://localhost:9222', help='Chrome CDP URL (default: http://localhost:9222)')

    args = parser.parse_args()

    asyncio.run(process_multiple_cases(
        json_path=args.json,
        db_path=args.db,
        downloads_dir=args.downloads,
        documents_dir=args.documents,
        start_index=args.start_index,
        max_cases=args.max_cases,
        cdp_url=args.cdp_url
    ))


if __name__ == "__main__":
    main()
