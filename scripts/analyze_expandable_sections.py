#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –±–ª–æ–∫–æ–≤ –∏–Ω—Å—Ç–∞–Ω—Ü–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –¥–µ–ª–∞.

–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —Å–µ–∫—Ü–∏–∏, —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –∏—Ö –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ.
"""

import asyncio
import json
from pathlib import Path

from src.scraper.playwright_scraper import PlaywrightScraper


async def analyze_expandable_sections():
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –±–ª–æ–∫–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –¥–µ–ª–∞."""

    print("=" * 80)
    print("–ê–ù–ê–õ–ò–ó –†–ê–°–ö–†–´–í–ê–Æ–©–ò–•–°–Ø –ë–õ–û–ö–û–í –ù–ê –°–¢–†–ê–ù–ò–¶–ï –î–ï–õ–ê")
    print("=" * 80)
    print()

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–¥–Ω–æ –¥–µ–ª–æ
    cases_file = Path("data/january_2024_cases.json")
    if not cases_file.exists():
        print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞ –ø–∞—Ä—Å–∏–Ω–≥.")
        return

    with open(cases_file, encoding="utf-8") as f:
        all_cases = json.load(f)

    # –í–∑—è—Ç—å –ø–µ—Ä–≤–æ–µ –¥–µ–ª–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞–∫—Ç–∞–º–∏
    case = all_cases[0]
    case_url = f"https://kad.arbitr.ru{case['url']}"

    print(f"üìã –î–µ–ª–æ: {case['case_number']}")
    print(f"üîó URL: {case_url}")
    print()

    async with PlaywrightScraper(use_cdp=True, cdp_url="http://localhost:9222") as scraper:
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Chrome\n")

        # –û—Ç–∫—Ä—ã—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–µ–ª–∞
        await scraper.page.goto(case_url, wait_until="networkidle", timeout=30000)
        await asyncio.sleep(3)
        print("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–µ–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")

        # ================================================================
        # 1. –ü–û–ò–°–ö –†–ê–°–ö–†–´–í–ê–Æ–©–ò–•–°–Ø –≠–õ–ï–ú–ï–ù–¢–û–í
        # ================================================================

        print("=" * 80)
        print("1. –ü–û–ò–°–ö –†–ê–°–ö–†–´–í–ê–Æ–©–ò–•–°–Ø –≠–õ–ï–ú–ï–ù–¢–û–í")
        print("=" * 80)
        print()

        # –í–æ–∑–º–æ–∂–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –±–ª–æ–∫–æ–≤
        selectors = [
            "button.toggle",
            "button.expand",
            "a.toggle",
            "div.collapsible",
            ".accordion",
            "[data-toggle]",
            "button:has-text('–ü–µ—Ä–≤–∞—è –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è')",
            "button:has-text('–ê–ø–µ–ª–ª—è—Ü–∏—è')",
            "button:has-text('–ö–∞—Å—Å–∞—Ü–∏—è')",
            "div:has-text('–ü–µ—Ä–≤–∞—è –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è') button",
            "div:has-text('–ê–ø–µ–ª–ª—è—Ü–∏—è') button",
        ]

        found_expandable = []

        for selector in selectors:
            elements = await scraper.page.query_selector_all(selector)
            if elements:
                print(f"‚úì –ù–∞–π–¥–µ–Ω–æ: {selector} ({len(elements)} —à—Ç.)")
                for el in elements[:3]:
                    text = await el.inner_text()
                    tag = await el.evaluate("el => el.tagName")
                    class_name = await el.get_attribute("class")
                    found_expandable.append({
                        "selector": selector,
                        "tag": tag,
                        "text": text.strip()[:50],
                        "class": class_name,
                        "element": el,
                    })

        print(f"\nüìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(found_expandable)}\n")

        # ================================================================
        # 2. –ü–û–ü–´–¢–ö–ê –†–ê–°–ö–†–´–¢–¨ –ë–õ–û–ö–ò
        # ================================================================

        print("=" * 80)
        print("2. –†–ê–°–ö–†–´–¢–ò–ï –ë–õ–û–ö–û–í")
        print("=" * 80)
        print()

        # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∫–ª–∏–∫–Ω—É—Ç—å –Ω–∞ –ø–µ—Ä–≤—ã–µ 3 —ç–ª–µ–º–µ–Ω—Ç–∞
        for i, item in enumerate(found_expandable[:3], 1):
            print(f"[{i}] –ü—Ä–æ–±—É—é —Ä–∞—Å–∫—Ä—ã—Ç—å: {item['text'][:40]}")
            try:
                # –°–∫—Ä–æ–ª–ª –∫ —ç–ª–µ–º–µ–Ω—Ç—É
                await item['element'].scroll_into_view_if_needed()
                await asyncio.sleep(0.5)

                # –ö–ª–∏–∫
                await item['element'].click()
                await asyncio.sleep(2)

                print(f"    ‚úÖ –ö–ª–∏–∫–Ω—É–ª")

                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –ø–æ—è–≤–∏–ª–∏—Å—å –ª–∏ –Ω–æ–≤—ã–µ PDF —Å—Å—ã–ª–∫–∏
                pdf_links = await scraper.page.query_selector_all('a[href$=".pdf"]')
                print(f"    PDF —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(pdf_links)}")

            except Exception as e:
                print(f"    ‚ö†Ô∏è  –û—à–∏–±–∫–∞: {e}")

        # ================================================================
        # 3. –ü–û–ò–°–ö –¢–ê–ë–õ–ò–¶ –° –ò–°–¢–û–†–ò–ï–ô
        # ================================================================

        print("\n" + "=" * 80)
        print("3. –ü–û–ò–°–ö –¢–ê–ë–õ–ò–¶ –° –ò–°–¢–û–†–ò–ï–ô –î–ï–õ–ê")
        print("=" * 80)
        print()

        tables = await scraper.page.query_selector_all("table")
        print(f"üìä –í—Å–µ–≥–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}\n")

        for i, table in enumerate(tables, 1):
            table_id = await table.get_attribute("id")
            table_class = await table.get_attribute("class")

            print(f"–¢–∞–±–ª–∏—Ü–∞ {i}:")
            print(f"   ID: {table_id or 'N/A'}")
            print(f"   Class: {table_class or 'N/A'}")

            # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ (–∑–∞–≥–æ–ª–æ–≤–∫–∏)
            rows = await table.query_selector_all("tr")
            if rows:
                first_row = rows[0]
                headers = await first_row.query_selector_all("th, td")
                if headers:
                    header_texts = []
                    for h in headers[:5]:
                        text = await h.inner_text()
                        header_texts.append(text.strip()[:20])
                    print(f"   –ó–∞–≥–æ–ª–æ–≤–∫–∏: {' | '.join(header_texts)}")

            print(f"   –°—Ç—Ä–æ–∫: {len(rows)}\n")

        # ================================================================
        # 4. –í–°–ï PDF –ù–ê –°–¢–†–ê–ù–ò–¶–ï
        # ================================================================

        print("=" * 80)
        print("4. –í–°–ï PDF –°–°–´–õ–ö–ò –ü–û–°–õ–ï –†–ê–°–ö–†–´–¢–ò–Ø")
        print("=" * 80)
        print()

        all_pdfs = await scraper.page.query_selector_all('a[href$=".pdf"]')
        print(f"üìÑ –í—Å–µ–≥–æ PDF —Å—Å—ã–ª–æ–∫: {len(all_pdfs)}\n")

        for i, link in enumerate(all_pdfs[:15], 1):
            text = await link.inner_text()
            href = await link.get_attribute("href")
            print(f"{i}. {text.strip()[:60]}")

        # ================================================================
        # 5. –°–û–•–†–ê–ù–ò–¢–¨ HTML
        # ================================================================

        print("\n" + "=" * 80)
        print("5. –°–û–•–†–ê–ù–ï–ù–ò–ï HTML")
        print("=" * 80)
        print()

        html = await scraper.page.content()
        html_file = Path("data/case_page_expanded.html")
        html_file.write_text(html, encoding="utf-8")
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {html_file}")
        print(f"   –†–∞–∑–º–µ—Ä: {len(html)} –±–∞–π—Ç")

        print("\n" + "=" * 80)
        print("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
        print("=" * 80)


if __name__ == "__main__":
    asyncio.run(analyze_expandable_sections())
