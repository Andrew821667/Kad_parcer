#!/usr/bin/env python3
"""
Test full workflow: parse 3 pages + download 5 court decisions.
"""

import asyncio
import random
from pathlib import Path

from structlog import get_logger

from src.scraper.playwright_scraper import PlaywrightScraper

logger = get_logger(__name__)


async def test_full_workflow():
    """Test complete workflow with document downloads."""
    print("üöÄ –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç workflow: –ø–∞—Ä—Å–∏–Ω–≥ + —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∞–∫—Ç–æ–≤\n")

    # Create scraper with CDP
    async with PlaywrightScraper(
        use_cdp=True,
        cdp_url="http://localhost:9222",
    ) as scraper:
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Chrome —á–µ—Ä–µ–∑ CDP\n")

        # STEP 1: Parse 3 pages
        print("=" * 80)
        print("–®–ê–ì 1: –ü–∞—Ä—Å–∏–Ω–≥ 3 —Å—Ç—Ä–∞–Ω–∏—Ü")
        print("=" * 80)

        results = []

        # Navigate and search
        await scraper.page.goto("https://kad.arbitr.ru", wait_until="networkidle")
        await asyncio.sleep(2)

        # Close popup
        try:
            await scraper.page.keyboard.press("Escape")
            await asyncio.sleep(1)
        except Exception:
            pass

        # Fill dates
        date_inputs = await scraper.page.query_selector_all(
            'input[placeholder="–¥–¥.–º–º.–≥–≥–≥–≥"]'
        )
        if len(date_inputs) >= 2:
            await date_inputs[0].click()
            await asyncio.sleep(0.2)
            await date_inputs[0].fill("01.01.2024")
            await asyncio.sleep(0.5)

            await date_inputs[1].click()
            await asyncio.sleep(0.2)
            await date_inputs[1].fill("31.01.2024")
            await asyncio.sleep(0.5)

        await scraper.page.click("body")
        await asyncio.sleep(0.5)

        # Submit
        await scraper.page.click("#b-form-submit")
        await asyncio.sleep(5)

        # Get pages count
        total_pages_input = await scraper.page.query_selector(
            "input#documentsPagesCount"
        )
        if not total_pages_input:
            print("‚ùå –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return

        total_pages_str = await total_pages_input.get_attribute("value")
        total_pages = int(total_pages_str) if total_pages_str else 0

        print(f"\nüìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
        print(f"üìÑ –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å: 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n")

        # Parse first 3 pages
        for page_num in range(1, min(4, total_pages + 1)):
            print(f"üìñ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}/3...")

            # Navigate to page (skip for first)
            if page_num > 1:
                link = await scraper.page.query_selector(f'a[href="#page{page_num}"]')
                if link:
                    await link.click()
                    await asyncio.sleep(5)

            # Parse current page
            page_cases = await scraper._parse_current_page()
            results.extend(page_cases)
            print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ –¥–µ–ª: {len(page_cases)}")

        print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(results)} –¥–µ–ª\n")

        # STEP 2: Select 5 random cases
        print("=" * 80)
        print("–®–ê–ì 2: –í—ã–±–æ—Ä 5 —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–ª –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∞–∫—Ç–æ–≤")
        print("=" * 80)

        if len(results) < 5:
            print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–µ–ª ({len(results)}), –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 5")
            return

        selected_cases = random.sample(results, 5)

        print("\nüìã –í—ã–±—Ä–∞–Ω–Ω—ã–µ –¥–µ–ª–∞:")
        for i, case in enumerate(selected_cases, 1):
            print(f"{i}. {case['case_number']} - {case['case_date']}")
            print(f"   URL: {case['url']}")

        # STEP 3: Download documents
        print("\n" + "=" * 80)
        print("–®–ê–ì 3: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤")
        print("=" * 80)

        # Setup download directory
        downloads_dir = Path.home() / "Downloads" / "kad_test"
        downloads_dir.mkdir(parents=True, exist_ok=True)

        print(f"\nüìÅ –ü–∞–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {downloads_dir}\n")

        downloaded_count = 0

        for i, case in enumerate(selected_cases, 1):
            print(f"üìÑ –î–µ–ª–æ {i}/5: {case['case_number']}")

            try:
                # Open case page in same tab
                await scraper.page.goto(case["url"], wait_until="networkidle")
                await asyncio.sleep(2)

                print(f"   ‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–µ–ª–∞ –æ—Ç–∫—Ä—ã—Ç–∞")

                # Look for document download links
                # Common patterns on kad.arbitr.ru:
                # - Links with text "–°—É–¥–µ–±–Ω—ã–π –∞–∫—Ç"
                # - Links to PDF files
                # - Download buttons

                # Try to find document links
                doc_links = await scraper.page.query_selector_all(
                    'a[href*=".pdf"], a:has-text("–°—É–¥–µ–±–Ω—ã–π –∞–∫—Ç"), a:has-text("–°–∫–∞—á–∞—Ç—å"), a.btn-download'
                )

                if not doc_links:
                    print(f"   ‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
                    continue

                print(f"   –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã: {len(doc_links)}")

                # Try to download first document
                first_link = doc_links[0]
                link_text = await first_link.inner_text()
                link_href = await first_link.get_attribute("href")

                print(f"   –ü—ã—Ç–∞—é—Å—å —Å–∫–∞—á–∞—Ç—å: {link_text[:50]} ({link_href[:50]}...)")

                # Setup download event listener
                async with scraper.page.expect_download(timeout=30000) as download_info:
                    await first_link.click()

                download = await download_info.value

                # Save file
                filename = f"{case['case_number'].replace('/', '_')}_{download.suggested_filename}"
                filepath = downloads_dir / filename

                await download.save_as(str(filepath))

                file_size = filepath.stat().st_size if filepath.exists() else 0

                print(f"   ‚úÖ –°–∫–∞—á–∞–Ω: {filename} ({file_size} bytes)")
                downloaded_count += 1

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                continue

        # Summary
        print("\n" + "=" * 80)
        print("–ò–¢–û–ì–ò")
        print("=" * 80)
        print(f"‚úÖ –î–µ–ª —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(results)}")
        print(f"‚úÖ –ê–∫—Ç–æ–≤ —Å–∫–∞—á–∞–Ω–æ: {downloaded_count}/5")
        print(f"üìÅ –ü–∞–ø–∫–∞: {downloads_dir}")
        print("=" * 80)

        if downloaded_count > 0:
            print(f"\nüéâ –£—Å–ø–µ—à–Ω–æ! –û—Ç–∫—Ä–æ–π—Ç–µ –ø–∞–ø–∫—É –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã:")
            print(f"   open {downloads_dir}")


if __name__ == "__main__":
    print("‚ö†Ô∏è  –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Chrome –∑–∞–ø—É—â–µ–Ω —Å remote debugging!")
    print("   –ö–æ–º–∞–Ω–¥–∞: ./scripts/start_chrome_debug.sh\n")

    asyncio.run(test_full_workflow())
