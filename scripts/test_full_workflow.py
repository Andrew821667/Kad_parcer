#!/usr/bin/env python3
"""
Test full workflow: parse 3 pages + download 5 court decisions.
"""

import asyncio
import random
from pathlib import Path

from structlog import get_logger

from src.scraper.playwright_scraper import PlaywrightScraper

logger = get_logger(__name__)


async def test_full_workflow():
    """Test complete workflow with document downloads."""
    print("üöÄ –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç workflow: –ø–∞—Ä—Å–∏–Ω–≥ + —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∞–∫—Ç–æ–≤\n")

    # Create scraper with CDP
    async with PlaywrightScraper(
        use_cdp=True,
        cdp_url="http://localhost:9222",
    ) as scraper:
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Chrome —á–µ—Ä–µ–∑ CDP\n")

        # STEP 1: Parse 3 pages
        print("=" * 80)
        print("–®–ê–ì 1: –ü–∞—Ä—Å–∏–Ω–≥ 3 —Å—Ç—Ä–∞–Ω–∏—Ü")
        print("=" * 80)

        results = []

        # Navigate and search
        await scraper.page.goto("https://kad.arbitr.ru", wait_until="networkidle")
        await asyncio.sleep(2)

        # Close popup
        try:
            await scraper.page.keyboard.press("Escape")
            await asyncio.sleep(1)
        except Exception:
            pass

        # Fill dates
        date_inputs = await scraper.page.query_selector_all(
            'input[placeholder="–¥–¥.–º–º.–≥–≥–≥–≥"]'
        )
        if len(date_inputs) >= 2:
            await date_inputs[0].click()
            await asyncio.sleep(0.2)
            await date_inputs[0].fill("01.01.2024")
            await asyncio.sleep(0.5)

            await date_inputs[1].click()
            await asyncio.sleep(0.2)
            await date_inputs[1].fill("31.01.2024")
            await asyncio.sleep(0.5)

        await scraper.page.click("body")
        await asyncio.sleep(0.5)

        # Submit
        await scraper.page.click("#b-form-submit")
        await asyncio.sleep(5)

        # Get pages count
        total_pages_input = await scraper.page.query_selector(
            "input#documentsPagesCount"
        )
        if not total_pages_input:
            print("‚ùå –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return

        total_pages_str = await total_pages_input.get_attribute("value")
        total_pages = int(total_pages_str) if total_pages_str else 0

        print(f"\nüìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
        print(f"üìÑ –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å: 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n")

        # Parse first 3 pages
        for page_num in range(1, min(4, total_pages + 1)):
            print(f"üìñ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}/3...")

            # Navigate to page (skip for first)
            if page_num > 1:
                link = await scraper.page.query_selector(f'a[href="#page{page_num}"]')
                if link:
                    await link.click()
                    await asyncio.sleep(5)

            # Parse current page
            page_cases = await scraper._parse_current_page()
            results.extend(page_cases)
            print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ –¥–µ–ª: {len(page_cases)}")

        print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(results)} –¥–µ–ª\n")

        # STEP 2: Select 5 random cases
        print("=" * 80)
        print("–®–ê–ì 2: –í—ã–±–æ—Ä 5 —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–ª –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∞–∫—Ç–æ–≤")
        print("=" * 80)

        if len(results) < 5:
            print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–µ–ª ({len(results)}), –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 5")
            return

        selected_cases = random.sample(results, 5)

        print("\nüìã –í—ã–±—Ä–∞–Ω–Ω—ã–µ –¥–µ–ª–∞:")
        for i, case in enumerate(selected_cases, 1):
            print(f"{i}. {case['case_number']} - {case['case_date']}")
            print(f"   URL: {case['url']}")

        # STEP 3: Download documents
        print("\n" + "=" * 80)
        print("–®–ê–ì 3: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤")
        print("=" * 80)

        # Setup download directory
        downloads_dir = Path.home() / "Downloads" / "kad_test"
        downloads_dir.mkdir(parents=True, exist_ok=True)

        print(f"\nüìÅ –ü–∞–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {downloads_dir}\n")

        downloaded_count = 0

        for i, case in enumerate(selected_cases, 1):
            print(f"üìÑ –î–µ–ª–æ {i}/5: {case['case_number']}")

            try:
                # Open case page in same tab
                await scraper.page.goto(case["url"], wait_until="networkidle")
                await asyncio.sleep(2)

                print(f"   ‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–µ–ª–∞ –æ—Ç–∫—Ä—ã—Ç–∞")

                # Look for DIRECT PDF links (ending with .pdf)
                doc_links = await scraper.page.query_selector_all('a[href$=".pdf"]')

                if not doc_links:
                    print(f"   ‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF")
                    continue

                print(f"   –ù–∞–π–¥–µ–Ω–æ PDF —Å—Å—ã–ª–æ–∫: {len(doc_links)}")

                # Get first PDF link
                first_link = doc_links[0]
                link_text = await first_link.inner_text()
                pdf_url = await first_link.get_attribute("href")

                print(f"   –î–æ–∫—É–º–µ–Ω—Ç: {link_text[:50]}")
                print(f"   URL: {pdf_url}")

                # Extract filename from URL
                pdf_filename = pdf_url.split("/")[-1] if pdf_url else "document.pdf"

                # Method: Open link in new tab and download
                try:
                    # Create new page for download
                    new_page = await scraper.page.context.new_page()

                    # Set up download handler
                    async with new_page.expect_download(timeout=30000) as download_info:
                        # Navigate to PDF URL
                        await new_page.goto(pdf_url, wait_until="load")

                    download = await download_info.value

                    # Save file
                    filename = f"{case['case_number'].replace('/', '_')}_{pdf_filename}"
                    filepath = downloads_dir / filename

                    await download.save_as(str(filepath))

                    file_size = filepath.stat().st_size if filepath.exists() else 0

                    print(f"   ‚úÖ –°–∫–∞—á–∞–Ω: {filename} ({file_size} bytes)")
                    downloaded_count += 1

                    # Close new page
                    await new_page.close()

                except Exception as download_error:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞: {download_error}")

                    # Try alternative: download via goto and content
                    try:
                        print(f"   –ü—Ä–æ–±—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥...")

                        # Create new page
                        new_page = await scraper.page.context.new_page()

                        # Navigate to PDF
                        await new_page.goto(pdf_url, wait_until="load", timeout=30000)
                        await asyncio.sleep(2)

                        # Try to get PDF content
                        pdf_content = await new_page.content()

                        # Save as HTML (contains PDF viewer)
                        filename = f"{case['case_number'].replace('/', '_')}_{pdf_filename}"
                        filepath = downloads_dir / filename

                        # If page loaded PDF, try to get it as bytes
                        try:
                            # Check if it's a PDF page (Chrome PDF viewer)
                            is_pdf_viewer = "pdf" in new_page.url.lower()

                            if is_pdf_viewer:
                                # Use CDP to get PDF
                                client = await new_page.context.new_cdp_session(new_page)
                                result = await client.send(
                                    "Page.printToPDF", {"printBackground": True}
                                )
                                import base64

                                pdf_bytes = base64.b64decode(result["data"])
                                filepath.write_bytes(pdf_bytes)

                                print(
                                    f"   ‚úÖ –°–∫–∞—á–∞–Ω (–∞–ª—å—Ç): {filename} ({len(pdf_bytes)} bytes)"
                                )
                                downloaded_count += 1
                            else:
                                print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å PDF")

                        except Exception as e:
                            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∞–ª—å—Ç. –º–µ—Ç–æ–¥–∞: {e}")

                        # Close new page
                        await new_page.close()

                    except Exception as alt_error:
                        print(f"   ‚ùå –ê–ª—å—Ç. –º–µ—Ç–æ–¥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {alt_error}")
                        continue

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                continue

        # Summary
        print("\n" + "=" * 80)
        print("–ò–¢–û–ì–ò")
        print("=" * 80)
        print(f"‚úÖ –î–µ–ª —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(results)}")
        print(f"‚úÖ –ê–∫—Ç–æ–≤ —Å–∫–∞—á–∞–Ω–æ: {downloaded_count}/5")
        print(f"üìÅ –ü–∞–ø–∫–∞: {downloads_dir}")
        print("=" * 80)

        if downloaded_count > 0:
            print(f"\nüéâ –£—Å–ø–µ—à–Ω–æ! –û—Ç–∫—Ä–æ–π—Ç–µ –ø–∞–ø–∫—É –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã:")
            print(f"   open {downloads_dir}")


if __name__ == "__main__":
    print("‚ö†Ô∏è  –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Chrome –∑–∞–ø—É—â–µ–Ω —Å remote debugging!")
    print("   –ö–æ–º–∞–Ω–¥–∞: ./scripts/start_chrome_debug.sh\n")

    asyncio.run(test_full_workflow())
