#!/usr/bin/env python3
"""
Test full workflow: parse 3 pages + download 5 court decisions.
"""

import asyncio
import random
from pathlib import Path

from structlog import get_logger

from src.scraper.playwright_scraper import PlaywrightScraper

logger = get_logger(__name__)


async def test_full_workflow():
    """Test complete workflow with document downloads."""
    print("üöÄ –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç workflow: –ø–∞—Ä—Å–∏–Ω–≥ + —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∞–∫—Ç–æ–≤\n")

    # Create scraper with CDP
    async with PlaywrightScraper(
        use_cdp=True,
        cdp_url="http://localhost:9222",
    ) as scraper:
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Chrome —á–µ—Ä–µ–∑ CDP\n")

        # STEP 1: Parse 3 pages
        print("=" * 80)
        print("–®–ê–ì 1: –ü–∞—Ä—Å–∏–Ω–≥ 3 —Å—Ç—Ä–∞–Ω–∏—Ü")
        print("=" * 80)

        results = []

        # Navigate and search
        await scraper.page.goto("https://kad.arbitr.ru", wait_until="networkidle")
        await asyncio.sleep(2)

        # Close popup
        try:
            await scraper.page.keyboard.press("Escape")
            await asyncio.sleep(1)
        except Exception:
            pass

        # Fill dates
        date_inputs = await scraper.page.query_selector_all(
            'input[placeholder="–¥–¥.–º–º.–≥–≥–≥–≥"]'
        )
        if len(date_inputs) >= 2:
            await date_inputs[0].click()
            await asyncio.sleep(0.2)
            await date_inputs[0].fill("01.01.2024")
            await asyncio.sleep(0.5)

            await date_inputs[1].click()
            await asyncio.sleep(0.2)
            await date_inputs[1].fill("31.01.2024")
            await asyncio.sleep(0.5)

        await scraper.page.click("body")
        await asyncio.sleep(0.5)

        # Submit
        await scraper.page.click("#b-form-submit")
        await asyncio.sleep(5)

        # Get pages count
        total_pages_input = await scraper.page.query_selector(
            "input#documentsPagesCount"
        )
        if not total_pages_input:
            print("‚ùå –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return

        total_pages_str = await total_pages_input.get_attribute("value")
        total_pages = int(total_pages_str) if total_pages_str else 0

        print(f"\nüìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
        print(f"üìÑ –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å: 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n")

        # Parse first 3 pages
        for page_num in range(1, min(4, total_pages + 1)):
            print(f"üìñ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}/3...")

            # Navigate to page (skip for first)
            if page_num > 1:
                link = await scraper.page.query_selector(f'a[href="#page{page_num}"]')
                if link:
                    await link.click()
                    await asyncio.sleep(5)

            # Parse current page
            page_cases = await scraper._parse_current_page()
            results.extend(page_cases)
            print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ –¥–µ–ª: {len(page_cases)}")

        print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(results)} –¥–µ–ª\n")

        # STEP 2: Select 5 random cases
        print("=" * 80)
        print("–®–ê–ì 2: –í—ã–±–æ—Ä 5 —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–ª –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∞–∫—Ç–æ–≤")
        print("=" * 80)

        if len(results) < 5:
            print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–µ–ª ({len(results)}), –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 5")
            return

        selected_cases = random.sample(results, 5)

        print("\nüìã –í—ã–±—Ä–∞–Ω–Ω—ã–µ –¥–µ–ª–∞:")
        for i, case in enumerate(selected_cases, 1):
            print(f"{i}. {case['case_number']} - {case['case_date']}")
            print(f"   URL: {case['url']}")

        # STEP 3: Download documents
        print("\n" + "=" * 80)
        print("–®–ê–ì 3: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤")
        print("=" * 80)

        # Setup download directory
        downloads_dir = Path.home() / "Downloads" / "kad_test"
        downloads_dir.mkdir(parents=True, exist_ok=True)

        print(f"\nüìÅ –ü–∞–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {downloads_dir}\n")

        downloaded_count = 0

        for i, case in enumerate(selected_cases, 1):
            print(f"üìÑ –î–µ–ª–æ {i}/5: {case['case_number']}")

            try:
                # Open case page in same tab
                case_url = f"https://kad.arbitr.ru{case['url']}"
                await scraper.page.goto(case_url, wait_until="networkidle")
                await asyncio.sleep(2)

                print(f"   ‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–µ–ª–∞ –æ—Ç–∫—Ä—ã—Ç–∞")

                # Look for DIRECT PDF links (ending with .pdf)
                doc_links = await scraper.page.query_selector_all('a[href$=".pdf"]')

                if not doc_links:
                    print(f"   ‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF")
                    continue

                print(f"   –ù–∞–π–¥–µ–Ω–æ PDF —Å—Å—ã–ª–æ–∫: {len(doc_links)}")

                # Get cookies once for all downloads
                cookies = await scraper.page.context.cookies()
                cookie_dict = {cookie['name']: cookie['value'] for cookie in cookies}

                # Download ALL PDFs from this case
                import httpx

                for doc_idx, link in enumerate(doc_links, 1):
                    try:
                        link_text = await link.inner_text()
                        pdf_url = await link.get_attribute("href")

                        print(f"   [{doc_idx}/{len(doc_links)}] {link_text[:50]}")

                        # Extract filename from URL
                        pdf_filename = pdf_url.split("/")[-1] if pdf_url else f"document_{doc_idx}.pdf"

                        # Download PDF via HTTP with browser cookies
                        async with httpx.AsyncClient(
                            cookies=cookie_dict,
                            timeout=30.0,
                            follow_redirects=True
                        ) as client:
                            response = await client.get(pdf_url)

                            if response.status_code == 200:
                                # Verify it's actually a PDF
                                content_type = response.headers.get('content-type', '')

                                if 'pdf' in content_type.lower() or pdf_url.endswith('.pdf'):
                                    # Save PDF with index to avoid overwriting
                                    filename = f"{case['case_number'].replace('/', '_')}_{doc_idx}_{pdf_filename}"
                                    filepath = downloads_dir / filename

                                    filepath.write_bytes(response.content)

                                    print(f"       ‚úÖ {len(response.content)//1024} KB")
                                    downloaded_count += 1
                                else:
                                    print(f"       ‚ö†Ô∏è  –ù–µ PDF (Content-Type: {content_type})")
                            else:
                                print(f"       ‚ùå HTTP {response.status_code}")

                    except Exception as download_error:
                        print(f"       ‚ùå –û—à–∏–±–∫–∞: {download_error}")
                        continue

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                continue

        # Summary
        print("\n" + "=" * 80)
        print("–ò–¢–û–ì–ò")
        print("=" * 80)
        print(f"‚úÖ –î–µ–ª —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(results)}")
        print(f"‚úÖ –î–µ–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 5")
        print(f"‚úÖ –ê–∫—Ç–æ–≤ —Å–∫–∞—á–∞–Ω–æ: {downloaded_count}")
        print(f"üìÅ –ü–∞–ø–∫–∞: {downloads_dir}")
        print("=" * 80)

        if downloaded_count > 0:
            print(f"\nüéâ –£—Å–ø–µ—à–Ω–æ! –û—Ç–∫—Ä–æ–π—Ç–µ –ø–∞–ø–∫—É –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã:")
            print(f"   open {downloads_dir}")


if __name__ == "__main__":
    print("‚ö†Ô∏è  –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Chrome –∑–∞–ø—É—â–µ–Ω —Å remote debugging!")
    print("   –ö–æ–º–∞–Ω–¥–∞: ./scripts/start_chrome_debug.sh\n")

    asyncio.run(test_full_workflow())
