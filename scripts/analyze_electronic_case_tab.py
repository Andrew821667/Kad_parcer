#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –≤–∫–ª–∞–¥–∫–∏ "–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ" –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –¥–µ–ª–∞ –ö–ê–î –ê—Ä–±–∏—Ç—Ä.
–¶–µ–ª—å: –Ω–∞–π—Ç–∏ –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –ø–æ–Ω—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.
"""

import asyncio
import json
from pathlib import Path

from src.scraper.playwright_scraper import PlaywrightScraper


async def analyze_electronic_case_tab():
    """–ê–Ω–∞–ª–∏–∑ –≤–∫–ª–∞–¥–∫–∏ –≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ."""

    print("=" * 80)
    print("–ê–ù–ê–õ–ò–ó –í–ö–õ–ê–î–ö–ò '–≠–õ–ï–ö–¢–†–û–ù–ù–û–ï –î–ï–õ–û'")
    print("=" * 80)
    print()

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤–æ–µ –¥–µ–ª–æ
    cases_file = Path("data/january_2024_cases.json")
    with open(cases_file, encoding="utf-8") as f:
        all_cases = json.load(f)

    case = all_cases[0]
    case_url = case['url']

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
    case_url = case_url.replace('https//kad.arbitr.ru', '').replace('http//kad.arbitr.ru', '').replace('//kad.arbitr.ru', '').replace('https://kad.arbitr.ru', '').replace('http://kad.arbitr.ru', '').replace('https:/', '').replace('http:/', '')
    if not case_url.startswith('/'):
        case_url = '/' + case_url
    case_url = f"https://kad.arbitr.ru{case_url}"

    print(f"üìã –î–µ–ª–æ: {case['case_number']}")
    print(f"üîó URL: {case_url}\n")

    async with PlaywrightScraper(use_cdp=True, cdp_url="http://localhost:9222") as scraper:
        await scraper.page.goto(case_url, wait_until="networkidle", timeout=30000)
        await asyncio.sleep(2)

        print("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–µ–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")

        # ================================================================
        # 1. –ù–ê–ô–¢–ò –ò –ò–ó–£–ß–ò–¢–¨ –í–ö–õ–ê–î–ö–ò
        # ================================================================

        print("=" * 80)
        print("1. –ü–û–ò–°–ö –í–ö–õ–ê–î–û–ö")
        print("=" * 80)
        print()

        # –ü–†–ê–í–ò–õ–¨–ù–´–ô –°–ï–õ–ï–ö–¢–û–†: .js-case-chrono-button--ed
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
        # <div class="b-case-chrono-button js-case-chrono-button js-case-chrono-button--ed">
        #     <div class="b-case-chrono-button-text">–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ</div>
        # </div>

        electronic_tab = await scraper.page.query_selector(".js-case-chrono-button--ed")

        if electronic_tab:
            tag = await electronic_tab.evaluate("el => el.tagName")
            classes = await electronic_tab.get_attribute("class") or ""
            text = await electronic_tab.inner_text()
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –≤–∫–ª–∞–¥–∫–∞ '–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ'")
            print(f"   Tag: <{tag}>")
            print(f"   Class: {classes}")
            print(f"   Text: {text.strip()}")
            print()
        else:
            print("‚ùå –í–∫–ª–∞–¥–∫–∞ '–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            print("   –î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –≤—Å–µ –≤–∫–ª–∞–¥–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ:\n")

            # –ù–∞–π—Ç–∏ –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∫–ª–∞–¥–∫–∏
            all_tabs = await scraper.page.query_selector_all("a, button, [role='tab'], .tab, li")
            print(f"–í—Å–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ –≤–∫–ª–∞–¥–∫–∏: {len(all_tabs)}\n")

            for i, tab in enumerate(all_tabs[:20], 1):
                try:
                    text = await tab.inner_text()
                    if text.strip() and len(text) < 100:
                        tag = await tab.evaluate("el => el.tagName")
                        classes = await tab.get_attribute("class") or ""
                        print(f"{i}. <{tag}> class='{classes[:40]}': {text.strip()[:50]}")
                except:
                    pass

            return

        # ================================================================
        # 2. –ö–õ–ò–ö–ù–£–¢–¨ –ù–ê –í–ö–õ–ê–î–ö–£ "–≠–õ–ï–ö–¢–†–û–ù–ù–û–ï –î–ï–õ–û"
        # ================================================================

        print("=" * 80)
        print("2. –ü–ï–†–ï–•–û–î –ù–ê –í–ö–õ–ê–î–ö–£ '–≠–õ–ï–ö–¢–†–û–ù–ù–û–ï –î–ï–õ–û'")
        print("=" * 80)
        print()

        try:
            await electronic_tab.click()
            print("‚úÖ –ö–ª–∏–∫–Ω—É–ª–∏ –Ω–∞ –≤–∫–ª–∞–¥–∫—É")
            await asyncio.sleep(3)  # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤–∫–ª–∞–¥–∫–∏
            print("‚úÖ –í–∫–ª–∞–¥–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∏–∫–µ: {e}\n")
            return

        # ================================================================
        # 3. –ù–ê–ô–¢–ò –í–°–ï PDF –î–û–ö–£–ú–ï–ù–¢–´ –ù–ê –í–ö–õ–ê–î–ö–ï
        # ================================================================

        print("=" * 80)
        print("3. –ü–û–ò–°–ö PDF –î–û–ö–£–ú–ï–ù–¢–û–í")
        print("=" * 80)
        print()

        # –ù–∞–π—Ç–∏ –≤—Å–µ PDF —Å—Å—ã–ª–∫–∏
        pdf_links = await scraper.page.query_selector_all('a[href$=".pdf"]')
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ PDF —Å—Å—ã–ª–æ–∫: {len(pdf_links)}\n")

        if pdf_links:
            print("–ü–µ—Ä–≤—ã–µ 10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
            for i, link in enumerate(pdf_links[:10], 1):
                try:
                    text = await link.inner_text()
                    href = await link.get_attribute("href")
                    print(f"  {i}. {text.strip()[:60]}")
                    print(f"     URL: {href[:80]}")
                except:
                    pass
            print()

        # ================================================================
        # 4. –ü–û–ò–°–ö –ü–ê–ì–ò–ù–ê–¶–ò–ò
        # ================================================================

        print("=" * 80)
        print("4. –ü–û–ò–°–ö –ü–ê–ì–ò–ù–ê–¶–ò–ò")
        print("=" * 80)
        print()

        # –ò—Å–∫–∞—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        pagination_selectors = [
            ".pagination",
            "[class*='pag']",
            "a:has-text('–°–ª–µ–¥—É—é—â–∞—è')",
            "button:has-text('–°–ª–µ–¥—É—é—â–∞—è')",
            "a:has-text('>')",
            "[class*='next']",
            "[class*='page']",
        ]

        pagination_found = False
        for selector in pagination_selectors:
            try:
                elements = await scraper.page.query_selector_all(selector)
                if elements:
                    print(f"‚úì –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ ({selector}): {len(elements)}")

                    for i, elem in enumerate(elements[:5], 1):
                        try:
                            tag = await elem.evaluate("el => el.tagName")
                            text = await elem.inner_text()
                            classes = await elem.get_attribute("class") or ""
                            href = await elem.get_attribute("href") or ""

                            print(f"   {i}. <{tag}> class='{classes[:40]}'")
                            print(f"      Text: {text.strip()[:40]}")
                            if href:
                                print(f"      Href: {href[:60]}")
                        except:
                            pass
                    print()
                    pagination_found = True
            except:
                pass

        if not pagination_found:
            print("‚ùå –≠–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            print("   –í–æ–∑–º–æ–∂–Ω–æ –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ\n")

        # ================================================================
        # 5. –°–û–•–†–ê–ù–ò–¢–¨ HTML –í–ö–õ–ê–î–ö–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê
        # ================================================================

        print("=" * 80)
        print("5. –°–û–•–†–ê–ù–ï–ù–ò–ï HTML")
        print("=" * 80)
        print()

        # –ù–∞–π—Ç–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –≤–∫–ª–∞–¥–∫–∏
        # –û–±—ã—á–Ω–æ —ç—Ç–æ div —Å id –∏–ª–∏ class —Å–æ–¥–µ—Ä–∂–∞—â–∏–º "electronic", "documents", "files" –∏ —Ç.–¥.
        tab_content = await scraper.page.query_selector("#electronic_case, .electronic-case, #documents, .documents, .tab-content, [role='tabpanel']")

        if tab_content:
            html = await tab_content.inner_html()
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –±–µ—Ä–µ–º –≤–µ—Å—å body
            html = await scraper.page.content()

        html_file = Path("data/electronic_case_tab.html")
        html_file.write_text(html, encoding="utf-8")
        print(f"üíæ HTML –≤–∫–ª–∞–¥–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {html_file}")
        print(f"   –†–∞–∑–º–µ—Ä: {len(html)} –±–∞–π—Ç\n")

        print("=" * 80)
        print("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
        print("=" * 80)


if __name__ == "__main__":
    asyncio.run(analyze_electronic_case_tab())
