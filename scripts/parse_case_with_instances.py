#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ–ª–∞ —Å–æ –≤—Å–µ–º–∏ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è–º–∏.

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
1. –ù–∞—Ö–æ–¥–∏—Ç –±–ª–æ–∫ —Å —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–µ–π (#chrono_list_content)
2. –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –∏–Ω—Å—Ç–∞–Ω—Ü–∏–∏ (–ø–µ—Ä–≤–∞—è, –∞–ø–µ–ª–ª—è—Ü–∏—è, –∫–∞—Å—Å–∞—Ü–∏—è)
3. –†–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –∫–∞–∂–¥—É—é –∏–Ω—Å—Ç–∞–Ω—Ü–∏—é
4. –°–∫–∞—á–∏–≤–∞–µ—Ç –í–°–ï PDF –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∫–∞–∂–¥–æ–π –∏–Ω—Å—Ç–∞–Ω—Ü–∏–∏
5. –ü–∞—Ä—Å–∏—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–µ–ª–∞ (—Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—é)
6. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
"""

import asyncio
import json
from pathlib import Path
from typing import Any, List, Dict

import httpx
from structlog import get_logger

from src.scraper.playwright_scraper import PlaywrightScraper

logger = get_logger(__name__)


async def parse_case_with_instances():
    """–ü–∞—Ä—Å–∏–Ω–≥ –¥–µ–ª–∞ —Å–æ –≤—Å–µ–º–∏ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è–º–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏."""

    print("=" * 80)
    print("–ü–û–õ–ù–û–¶–ï–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì –î–ï–õ–ê –°–û –í–°–ï–ú–ò –ò–ù–°–¢–ê–ù–¶–ò–Ø–ú–ò")
    print("=" * 80)
    print()

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–¥–Ω–æ –¥–µ–ª–æ –¥–ª—è —Ç–µ—Å—Ç–∞
    cases_file = Path("data/january_2024_cases.json")
    with open(cases_file, encoding="utf-8") as f:
        all_cases = json.load(f)

    case = all_cases[0]
    case_url = case['url']

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
    case_url = case_url.replace('https//kad.arbitr.ru', '').replace('http//kad.arbitr.ru', '').replace('//kad.arbitr.ru', '').replace('https://kad.arbitr.ru', '').replace('http://kad.arbitr.ru', '').replace('https:/', '').replace('http:/', '')
    if not case_url.startswith('/'):
        case_url = '/' + case_url
    case_url = f"https://kad.arbitr.ru{case_url}"

    print(f"üìã –î–µ–ª–æ: {case['case_number']}")
    print(f"üîó URL: {case_url}\n")

    # –°–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    downloads_dir = Path("downloads") / case['case_number'].replace('/', '_')
    downloads_dir.mkdir(parents=True, exist_ok=True)

    async with PlaywrightScraper(use_cdp=True, cdp_url="http://localhost:9222") as scraper:
        await scraper.page.goto(case_url, wait_until="networkidle", timeout=30000)
        await asyncio.sleep(3)

        print("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–µ–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")

        # ================================================================
        # 1. –ù–ê–ô–¢–ò –ë–õ–û–ö –° –•–†–û–ù–û–õ–û–ì–ò–ï–ô
        # ================================================================

        print("=" * 80)
        print("–®–ê–ì 1: –ü–æ–∏—Å–∫ –±–ª–æ–∫–∞ —Å —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–µ–π –∏ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è–º–∏")
        print("=" * 80)
        print()

        chrono_block = await scraper.page.query_selector("#chrono_list_content")
        if not chrono_block:
            print("‚ùå –ë–ª–æ–∫ #chrono_list_content –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return

        print("‚úÖ –ë–ª–æ–∫ —Å —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–µ–π –Ω–∞–π–¥–µ–Ω\n")

        # ================================================================
        # 2. –ù–ê–ô–¢–ò –í–°–ï –ò–ù–°–¢–ê–ù–¶–ò–ò
        # ================================================================

        print("=" * 80)
        print("–®–ê–ì 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∏–Ω—Å—Ç–∞–Ω—Ü–∏–π")
        print("=" * 80)
        print()

        # –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–Ω—Å—Ç–∞–Ω—Ü–∏–π
        instance_headers = await chrono_block.query_selector_all("h2, h3, .instance-header, [class*='instance']")

        # –¢–∞–∫–∂–µ –∏—Å–∫–∞—Ç—å –ø–æ —Ç–µ–∫—Å—Ç—É
        all_divs = await chrono_block.query_selector_all("div")
        instance_blocks = []

        for div in all_divs[:50]:  # –ü–µ—Ä–≤—ã–µ 50 div'–æ–≤
            try:
                text = await div.inner_text()
                text_lower = text.lower()

                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç–∞–Ω—Ü–∏–∏
                if any(keyword in text_lower for keyword in ['–ø–µ—Ä–≤–∞—è –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è', '–∞–ø–µ–ª–ª—è—Ü', '–∫–∞—Å—Å–∞—Ü', 'instance']):
                    classes = await div.get_attribute("class") or ""

                    # –ù–∞–π—Ç–∏ –≤—Å–µ PDF –≤–Ω—É—Ç—Ä–∏ —ç—Ç–æ–≥–æ –±–ª–æ–∫–∞
                    pdfs_in_block = await div.query_selector_all('a[href$=".pdf"]')

                    if pdfs_in_block or len(text.strip()) < 200:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å PDF –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç (–∑–∞–≥–æ–ª–æ–≤–æ–∫)
                        instance_blocks.append({
                            "element": div,
                            "text": text.strip()[:100],
                            "class": classes,
                            "pdf_count": len(pdfs_in_block),
                        })
            except:
                pass

        print(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –∏–Ω—Å—Ç–∞–Ω—Ü–∏–π: {len(instance_blocks)}\n")

        for i, block in enumerate(instance_blocks[:10], 1):
            print(f"{i}. {block['text'][:60]}")
            print(f"   Class: {block['class'][:50]}")
            print(f"   PDF –≤–Ω—É—Ç—Ä–∏: {block['pdf_count']}")
            print()

        # ================================================================
        # 3. –†–ê–°–ö–†–´–¢–¨ –í–°–ï –ë–õ–û–ö–ò (–µ—Å–ª–∏ –æ–Ω–∏ —Å–≤–µ—Ä–Ω—É—Ç—ã)
        # ================================================================

        print("=" * 80)
        print("–®–ê–ì 3: –†–∞—Å–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤")
        print("=" * 80)
        print()

        # –ù–∞–π—Ç–∏ –≤—Å–µ –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏/–∫–Ω–æ–ø–∫–∏ —Ä–∞—Å–∫—Ä—ã—Ç–∏—è
        expandable = await scraper.page.query_selector_all(
            "button.expand, button.toggle, a.toggle, [data-toggle], .collapsible-header, h2[onclick], h3[onclick]"
        )

        print(f"–ù–∞–π–¥–µ–Ω–æ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(expandable)}")

        for i, el in enumerate(expandable, 1):
            try:
                text = await el.inner_text()
                print(f"  [{i}] –†–∞—Å–∫—Ä—ã–≤–∞—é: {text.strip()[:40]}")

                await el.scroll_into_view_if_needed()
                await asyncio.sleep(0.3)
                await el.click()
                await asyncio.sleep(1)

                print(f"      ‚úÖ –†–∞—Å–∫—Ä—ã—Ç–æ")
            except Exception as e:
                print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞: {str(e)[:50]}")

        print()

        # ================================================================
        # 4. –°–ö–ê–ß–ê–¢–¨ –í–°–ï PDF –ò–ó –í–°–ï–• –ò–ù–°–¢–ê–ù–¶–ò–ô
        # ================================================================

        print("=" * 80)
        print("–®–ê–ì 4: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤—Å–µ—Ö PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print("=" * 80)
        print()

        # –ü–æ—Å–ª–µ —Ä–∞—Å–∫—Ä—ã—Ç–∏—è - –Ω–∞–π—Ç–∏ –í–°–ï PDF –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        all_pdf_links = await scraper.page.query_selector_all('a[href$=".pdf"]')
        print(f"üìÑ –í—Å–µ–≥–æ PDF —Å—Å—ã–ª–æ–∫: {len(all_pdf_links)}\n")

        # –ü–æ–ª—É—á–∏—Ç—å cookies –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        cookies = await scraper.page.context.cookies()
        cookie_dict = {cookie['name']: cookie['value'] for cookie in cookies}

        downloaded_docs = []

        for i, link in enumerate(all_pdf_links, 1):
            try:
                link_text = await link.inner_text()
                pdf_url = await link.get_attribute("href")

                print(f"[{i}/{len(all_pdf_links)}] {link_text.strip()[:60]}")

                # –°–∫–∞—á–∞—Ç—å PDF
                async with httpx.AsyncClient(
                    cookies=cookie_dict,
                    timeout=30.0,
                    follow_redirects=True
                ) as client:
                    response = await client.get(pdf_url)

                    if response.status_code == 200:
                        content_type = response.headers.get('content-type', '')

                        if 'pdf' in content_type.lower() or pdf_url.endswith('.pdf'):
                            # –ò–º—è —Ñ–∞–π–ª–∞
                            pdf_filename = pdf_url.split("/")[-1] if pdf_url else f"document_{i}.pdf"
                            filename = f"{i:03d}_{pdf_filename}"
                            filepath = downloads_dir / filename

                            filepath.write_bytes(response.content)

                            print(f"   ‚úÖ {len(response.content)//1024} KB ‚Üí {filename}")

                            downloaded_docs.append({
                                "index": i,
                                "title": link_text.strip(),
                                "filename": filename,
                                "url": pdf_url,
                                "size_bytes": len(response.content),
                            })
                        else:
                            print(f"   ‚ö†Ô∏è  –ù–µ PDF (Content-Type: {content_type})")
                    else:
                        print(f"   ‚ùå HTTP {response.status_code}")

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                continue

        print()

        # ================================================================
        # 5. –ü–ê–†–°–ò–ù–ì –ò–°–¢–û–†–ò–ò –î–ï–õ–ê
        # ================================================================

        print("=" * 80)
        print("–®–ê–ì 5: –ü–∞—Ä—Å–∏–Ω–≥ –∏—Å—Ç–æ—Ä–∏–∏ –¥–µ–ª–∞ (—Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—è)")
        print("=" * 80)
        print()

        # –ù–∞–π—Ç–∏ —Ç–∞–±–ª–∏—Ü—ã —Å —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–µ–π
        chrono_tables = await chrono_block.query_selector_all("table")
        print(f"–¢–∞–±–ª–∏—Ü –≤ –±–ª–æ–∫–µ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–∏: {len(chrono_tables)}\n")

        case_history = []

        for table_idx, table in enumerate(chrono_tables, 1):
            rows = await table.query_selector_all("tr")
            print(f"–¢–∞–±–ª–∏—Ü–∞ {table_idx}: {len(rows)} —Å—Ç—Ä–æ–∫")

            for row in rows[:5]:  # –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                cells = await row.query_selector_all("td, th")
                row_data = []

                for cell in cells:
                    text = await cell.inner_text()
                    row_data.append(text.strip())

                if row_data:
                    print(f"   {' | '.join(row_data[:5])}")
                    case_history.append(row_data)

            print()

        # ================================================================
        # 6. –°–û–•–†–ê–ù–ò–¢–¨ –ú–ï–¢–ê–î–ê–ù–ù–´–ï
        # ================================================================

        print("=" * 80)
        print("–®–ê–ì 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
        print("=" * 80)
        print()

        metadata = {
            "case_number": case['case_number'],
            "case_url": case_url,
            "total_documents": len(downloaded_docs),
            "documents": downloaded_docs,
            "case_history": case_history[:20],  # –ü–µ—Ä–≤—ã–µ 20 –∑–∞–ø–∏—Å–µ–π
        }

        metadata_file = downloads_dir / "metadata.json"
        metadata_file.write_text(
            json.dumps(metadata, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

        print(f"üíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_file}")
        print(f"üíæ –î–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {downloads_dir}")
        print()

        # ================================================================
        # –ò–¢–û–ì–ò
        # ================================================================

        print("=" * 80)
        print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 80)
        print()

        print(f"‚úÖ –î–µ–ª–æ: {case['case_number']}")
        print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–∫–∞—á–∞–Ω–æ: {len(downloaded_docs)}")
        print(f"‚úÖ –ó–∞–ø–∏—Å–µ–π –∏—Å—Ç–æ—Ä–∏–∏: {len(case_history)}")
        print(f"‚úÖ –ü–∞–ø–∫–∞: {downloads_dir}")
        print()

        print("=" * 80)
        print("üéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        print("=" * 80)


if __name__ == "__main__":
    asyncio.run(parse_case_with_instances())
