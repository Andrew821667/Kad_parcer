#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ–ª–∞.
–ò—â–µ—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —ç–ª–µ–º–µ–Ω—Ç—ã –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è–º.
"""

import asyncio
import json
from pathlib import Path

from src.scraper.playwright_scraper import PlaywrightScraper


async def deep_html_analysis():
    """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã."""

    print("=" * 80)
    print("–ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó HTML –°–¢–†–£–ö–¢–£–†–´ –°–¢–†–ê–ù–ò–¶–´ –î–ï–õ–ê")
    print("=" * 80)
    print()

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–µ–ª–æ
    cases_file = Path("data/january_2024_cases.json")
    with open(cases_file, encoding="utf-8") as f:
        all_cases = json.load(f)

    case = all_cases[0]
    case_url = case['url']

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
    case_url = case_url.replace('https//kad.arbitr.ru', '').replace('http//kad.arbitr.ru', '').replace('//kad.arbitr.ru', '').replace('https://kad.arbitr.ru', '').replace('http://kad.arbitr.ru', '').replace('https:/', '').replace('http:/', '')
    if not case_url.startswith('/'):
        case_url = '/' + case_url
    case_url = f"https://kad.arbitr.ru{case_url}"

    print(f"üìã –î–µ–ª–æ: {case['case_number']}")
    print(f"üîó URL: {case_url}\n")

    async with PlaywrightScraper(use_cdp=True, cdp_url="http://localhost:9222") as scraper:
        await scraper.page.goto(case_url, wait_until="networkidle", timeout=30000)
        await asyncio.sleep(3)

        print("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")

        # ================================================================
        # 1. –ü–û–ò–°–ö –í–°–ï–• –≠–õ–ï–ú–ï–ù–¢–û–í –° –¢–ï–ö–°–¢–û–ú "–ò–ù–°–¢–ê–ù–¶–ò–Ø"
        # ================================================================

        print("=" * 80)
        print("1. –≠–õ–ï–ú–ï–ù–¢–´ –° –¢–ï–ö–°–¢–û–ú '–ò–ù–°–¢–ê–ù–¶–ò–Ø'")
        print("=" * 80)
        print()

        elements = await scraper.page.query_selector_all("*")
        instance_elements = []

        for el in elements[:500]:  # –ü–µ—Ä–≤—ã–µ 500 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            try:
                text = await el.inner_text()
                if '–∏–Ω—Å—Ç–∞–Ω—Ü' in text.lower():
                    tag = await el.evaluate("el => el.tagName")
                    classes = await el.get_attribute("class") or ""
                    element_id = await el.get_attribute("id") or ""

                    instance_elements.append({
                        "tag": tag,
                        "id": element_id,
                        "class": classes,
                        "text": text[:100],
                    })
            except:
                pass

        print(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(instance_elements)}\n")
        for i, el in enumerate(instance_elements[:10], 1):
            print(f"{i}. <{el['tag']}> id='{el['id']}' class='{el['class']}'")
            print(f"   –¢–µ–∫—Å—Ç: {el['text'][:80]}")
            print()

        # ================================================================
        # 2. –í–°–ï –ö–õ–ò–ö–ê–ë–ï–õ–¨–ù–´–ï –≠–õ–ï–ú–ï–ù–¢–´
        # ================================================================

        print("=" * 80)
        print("2. –ö–õ–ò–ö–ê–ë–ï–õ–¨–ù–´–ï –≠–õ–ï–ú–ï–ù–¢–´ (–ö–ù–û–ü–ö–ò/–°–°–´–õ–ö–ò)")
        print("=" * 80)
        print()

        buttons = await scraper.page.query_selector_all("button, a.toggle, [role='button'], [onclick]")
        print(f"–í—Å–µ–≥–æ –∫–Ω–æ–ø–æ–∫/—Å—Å—ã–ª–æ–∫: {len(buttons)}\n")

        for i, btn in enumerate(buttons[:15], 1):
            try:
                text = await btn.inner_text()
                tag = await btn.evaluate("el => el.tagName")
                classes = await btn.get_attribute("class") or ""

                if text.strip():
                    print(f"{i}. <{tag}> class='{classes[:50]}'")
                    print(f"   –¢–µ–∫—Å—Ç: {text.strip()[:60]}")
                    print()
            except:
                pass

        # ================================================================
        # 3. –°–¢–†–£–ö–¢–£–†–ê –î–û–ö–£–ú–ï–ù–¢–û–í
        # ================================================================

        print("=" * 80)
        print("3. –°–¢–†–£–ö–¢–£–†–ê –ë–õ–û–ö–û–í –° –î–û–ö–£–ú–ï–ù–¢–ê–ú–ò")
        print("=" * 80)
        print()

        # –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ —Å PDF
        pdf_containers = await scraper.page.query_selector_all("div:has(a[href$='.pdf']), ul:has(a[href$='.pdf']), section:has(a[href$='.pdf'])")

        print(f"–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ —Å PDF: {len(pdf_containers)}\n")

        for i, container in enumerate(pdf_containers[:5], 1):
            tag = await container.evaluate("el => el.tagName")
            classes = await container.get_attribute("class") or ""

            # –°–∫–æ–ª—å–∫–æ PDF –≤–Ω—É—Ç—Ä–∏
            pdfs = await container.query_selector_all('a[href$=".pdf"]')

            print(f"{i}. <{tag}> class='{classes[:60]}'")
            print(f"   PDF –≤–Ω—É—Ç—Ä–∏: {len(pdfs)}")

            # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 3 PDF
            for j, pdf in enumerate(pdfs[:3], 1):
                try:
                    pdf_text = await pdf.inner_text()
                    print(f"      {j}. {pdf_text.strip()[:50]}")
                except:
                    pass
            print()

        # ================================================================
        # 4. JAVASCRIPT EVENTS
        # ================================================================

        print("=" * 80)
        print("4. –≠–õ–ï–ú–ï–ù–¢–´ –° JAVASCRIPT EVENTS")
        print("=" * 80)
        print()

        js_elements = await scraper.page.query_selector_all("[onclick], [data-toggle], [data-target]")
        print(f"–≠–ª–µ–º–µ–Ω—Ç–æ–≤ —Å JS events: {len(js_elements)}\n")

        for i, el in enumerate(js_elements[:10], 1):
            tag = await el.evaluate("el => el.tagName")
            onclick = await el.get_attribute("onclick") or ""
            data_toggle = await el.get_attribute("data-toggle") or ""
            data_target = await el.get_attribute("data-target") or ""

            print(f"{i}. <{tag}>")
            if onclick:
                print(f"   onclick: {onclick[:80]}")
            if data_toggle:
                print(f"   data-toggle: {data_toggle}")
            if data_target:
                print(f"   data-target: {data_target}")
            print()

        # ================================================================
        # 5. –°–û–•–†–ê–ù–ò–¢–¨ –î–ï–¢–ê–õ–¨–ù–´–ô HTML –§–†–ê–ì–ú–ï–ù–¢
        # ================================================================

        print("=" * 80)
        print("5. –°–û–•–†–ê–ù–ï–ù–ò–ï HTML")
        print("=" * 80)
        print()

        # –ù–∞–π—Ç–∏ –≥–ª–∞–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –¥–µ–ª–æ–º
        main_content = await scraper.page.query_selector("main, #content, .content, .case-content")
        if main_content:
            html_fragment = await main_content.inner_html()

            fragment_file = Path("data/case_main_content.html")
            fragment_file.write_text(html_fragment, encoding="utf-8")
            print(f"üíæ –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {fragment_file}")
            print(f"   –†–∞–∑–º–µ—Ä: {len(html_fragment)} –±–∞–π—Ç\n")

        full_html = await scraper.page.content()
        full_file = Path("data/case_full_page.html")
        full_file.write_text(full_html, encoding="utf-8")
        print(f"üíæ –ü–æ–ª–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {full_file}")
        print(f"   –†–∞–∑–º–µ—Ä: {len(full_html)} –±–∞–π—Ç")

        print("\n" + "=" * 80)
        print("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
        print("=" * 80)


if __name__ == "__main__":
    asyncio.run(deep_html_analysis())
