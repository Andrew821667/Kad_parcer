#!/usr/bin/env python3
"""
–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –í–°–ï–• –¥–µ–ª –∏–∑ january_2024_cases.json.
–ü—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –∫–∞–∂–¥–æ–º—É –¥–µ–ª—É, –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –≤–∫–ª–∞–¥–∫—É "–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ" –∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.
"""

import asyncio
import json
from pathlib import Path

import httpx

from src.scraper.playwright_scraper import PlaywrightScraper


async def download_case_documents(scraper, case, base_downloads_dir):
    """–°–∫–∞—á–∞—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–µ–ª–∞."""

    case_number = case['case_number']
    case_url = case['url']

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
    case_url = case_url.replace('https//kad.arbitr.ru', '').replace('http//kad.arbitr.ru', '').replace('//kad.arbitr.ru', '').replace('https://kad.arbitr.ru', '').replace('http://kad.arbitr.ru', '').replace('https:/', '').replace('http:/', '')
    if not case_url.startswith('/'):
        case_url = '/' + case_url
    case_url = f"https://kad.arbitr.ru{case_url}"

    print(f"\n{'=' * 80}")
    print(f"üìã –î–µ–ª–æ: {case_number}")
    print(f"üîó URL: {case_url}")
    print(f"{'=' * 80}\n")

    # –°–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É –¥–ª—è –¥–µ–ª–∞
    case_folder = base_downloads_dir / case_number.replace('/', '_')
    case_folder.mkdir(parents=True, exist_ok=True)

    try:
        # –û—Ç–∫—Ä—ã—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–µ–ª–∞
        await scraper.page.goto(case_url, wait_until="networkidle", timeout=30000)
        await asyncio.sleep(2)

        # –ù–∞–π—Ç–∏ –∏ –∫–ª–∏–∫–Ω—É—Ç—å –Ω–∞ –≤–∫–ª–∞–¥–∫—É "–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ"
        electronic_tab = await scraper.page.query_selector(".js-case-chrono-button--ed")

        if not electronic_tab:
            print("‚ùå –í–∫–ª–∞–¥–∫–∞ '–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–µ–ª–æ\n")
            return {"case_number": case_number, "status": "no_tab", "documents": 0}

        await electronic_tab.click()
        await asyncio.sleep(3)

        # –ü–æ–ª—É—á–∏—Ç—å cookies –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        cookies = await scraper.page.context.cookies()
        cookie_dict = {cookie['name']: cookie['value'] for cookie in cookies}

        total_downloaded = 0
        downloaded_urls = set()
        page_num = 1

        while True:
            # –ù–∞–π—Ç–∏ –≤—Å–µ PDF –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            pdf_links = await scraper.page.query_selector_all('a[href$=".pdf"]')

            if not pdf_links:
                break

            # –°–∫–∞—á–∞—Ç—å –∫–∞–∂–¥—ã–π PDF
            for link in pdf_links:
                try:
                    text = await link.inner_text()
                    href = await link.get_attribute("href")

                    # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
                    if href in downloaded_urls:
                        continue

                    downloaded_urls.add(href)

                    # –°–∫–∞—á–∞—Ç—å —Å retry
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            async with httpx.AsyncClient(
                                cookies=cookie_dict,
                                timeout=30.0,
                                follow_redirects=True
                            ) as client:
                                response = await client.get(href)

                                if response.status_code == 200:
                                    content_type = response.headers.get('content-type', '')

                                    if 'pdf' in content_type.lower() or href.endswith('.pdf'):
                                        filename = href.split("/")[-1]
                                        if not filename.endswith('.pdf'):
                                            filename += '.pdf'

                                        filepath = case_folder / f"{total_downloaded + 1:03d}_{filename}"
                                        filepath.write_bytes(response.content)

                                        total_downloaded += 1
                                        break
                                    else:
                                        break

                                else:
                                    if attempt < max_retries - 1:
                                        await asyncio.sleep(2)
                                    break

                        except Exception as e:
                            if attempt < max_retries - 1:
                                await asyncio.sleep(2)
                            else:
                                pass  # –¢–∏—Ö–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏ –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

                except Exception:
                    pass  # –¢–∏—Ö–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            next_button = await scraper.page.query_selector(".js-chrono-pagination-pager-item--arrow.next")
            if not next_button:
                next_button = await scraper.page.query_selector(".js-card-list_paginator-item.next")

            if not next_button:
                break

            try:
                is_disabled = await next_button.evaluate("el => el.classList.contains('disabled') || el.hasAttribute('disabled')")
                if is_disabled:
                    break
            except:
                pass

            try:
                await next_button.click()
                await asyncio.sleep(3)
                page_num += 1
            except:
                break

        print(f"‚úÖ –°–∫–∞—á–∞–Ω–æ: {total_downloaded} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return {"case_number": case_number, "status": "success", "documents": total_downloaded}

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–µ–ª–∞: {str(e)[:100]}")
        return {"case_number": case_number, "status": "error", "documents": 0, "error": str(e)[:200]}


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –¥–µ–ª–∞."""

    print("=" * 80)
    print("–ú–ê–°–°–û–í–û–ï –°–ö–ê–ß–ò–í–ê–ù–ò–ï –î–û–ö–£–ú–ï–ù–¢–û–í –ò–ó JANUARY 2024")
    print("=" * 80)
    print()

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–µ–ª
    cases_file = Path("data/january_2024_cases.json")
    if not cases_file.exists():
        print(f"‚ùå –§–∞–π–ª {cases_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    with open(cases_file, encoding="utf-8") as f:
        all_cases = json.load(f)

    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –¥–µ–ª: {len(all_cases)}\n")

    # –°–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    downloads_dir = Path("downloads")
    downloads_dir.mkdir(exist_ok=True)

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = []
    total_docs = 0
    success_count = 0
    error_count = 0

    async with PlaywrightScraper(use_cdp=True, cdp_url="http://localhost:9222") as scraper:
        for i, case in enumerate(all_cases, 1):
            print(f"\n[{i}/{len(all_cases)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–µ–ª–∞...")

            result = await download_case_documents(scraper, case, downloads_dir)
            results.append(result)

            if result["status"] == "success":
                success_count += 1
                total_docs += result["documents"]
            else:
                error_count += 1

            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –¥–µ–ª–∞–º–∏
            await asyncio.sleep(1)

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_file = downloads_dir / "download_results.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 80)
    print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 80)
    print()
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–µ–ª: {len(all_cases)}")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {success_count}")
    print(f"‚ùå –û—à–∏–±–æ–∫: {error_count}")
    print(f"üìÑ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–∫–∞—á–∞–Ω–æ: {total_docs}")
    print(f"üíæ –ü–∞–ø–∫–∞: {downloads_dir}")
    print(f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
    print()
    print("=" * 80)
    print("üéâ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())
