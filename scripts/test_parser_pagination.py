#!/usr/bin/env python3
"""
Test parser with CDP and pagination for 2-3 pages.
"""

import asyncio
from datetime import date

from structlog import get_logger

from src.scraper.playwright_scraper import PlaywrightScraper

logger = get_logger(__name__)


async def test_parser_with_pagination():
    """Test parser with real Chrome via CDP."""
    print("üöÄ –¢–µ—Å—Ç –ø–∞—Ä—Å–µ—Ä–∞ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ CDP\n")

    # Create scraper with CDP
    async with PlaywrightScraper(
        use_cdp=True,
        cdp_url="http://localhost:9222",
    ) as scraper:
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Chrome —á–µ—Ä–µ–∑ CDP\n")

        # Search for January 2024 (limited to first 3 pages for testing)
        print("üîç –ü–æ–∏—Å–∫ –¥–µ–ª –∑–∞ —è–Ω–≤–∞—Ä—å 2024...")
        print("   (–û–≥—Ä–∞–Ω–∏—á–∏–º 3 —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∞)\n")

        try:
            # Monkey-patch to limit pages for testing
            original_search = scraper.search_cases

            async def limited_search(*args, **kwargs):
                results = []
                # Do original form filling
                await scraper.page.goto("https://kad.arbitr.ru", wait_until="networkidle")
                await asyncio.sleep(2)

                # Close popup
                try:
                    await scraper.page.keyboard.press("Escape")
                    await asyncio.sleep(1)
                except Exception:
                    pass

                # Fill dates
                date_inputs = await scraper.page.query_selector_all('input[placeholder="–¥–¥.–º–º.–≥–≥–≥–≥"]')
                if len(date_inputs) >= 2:
                    await date_inputs[0].click()
                    await asyncio.sleep(0.2)
                    await date_inputs[0].fill("01.01.2024")
                    await asyncio.sleep(0.5)

                    await date_inputs[1].click()
                    await asyncio.sleep(0.2)
                    await date_inputs[1].fill("31.01.2024")
                    await asyncio.sleep(0.5)

                await scraper.page.click("body")
                await asyncio.sleep(0.5)

                # Submit
                await scraper.page.click("#b-form-submit")
                await asyncio.sleep(5)

                # Get pages count
                total_pages_input = await scraper.page.query_selector("input#documentsPagesCount")
                if not total_pages_input:
                    print("‚ùå –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                    return []

                total_pages_str = await total_pages_input.get_attribute("value")
                total_pages = int(total_pages_str) if total_pages_str else 0

                print(f"üìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
                print(f"üìÑ –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å: 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–¥–ª—è —Ç–µ—Å—Ç–∞)\n")

                # Parse first 3 pages
                for page_num in range(1, min(4, total_pages + 1)):
                    print(f"üìñ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}/3...")

                    # Navigate to page (skip for first)
                    if page_num > 1:
                        link = await scraper.page.query_selector(f'a[href="#page{page_num}"]')
                        if link:
                            await link.click()
                            await asyncio.sleep(5)
                            print(f"   ‚úì –ü–µ—Ä–µ—à–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}")
                        else:
                            print(f"   ‚úó –°—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                            continue

                    # Parse current page
                    page_cases = await scraper._parse_current_page()
                    results.extend(page_cases)
                    print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ –¥–µ–ª: {len(page_cases)}\n")

                return results

            # Run limited search
            cases = await limited_search()

            print("=" * 80)
            print(f"‚úÖ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù")
            print(f"   –í—Å–µ–≥–æ –¥–µ–ª —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(cases)}")
            print(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: ~75 –¥–µ–ª (3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã √ó 25)")
            print("=" * 80)

            if cases:
                print("\nüìã –ü–µ—Ä–≤—ã–µ 3 –¥–µ–ª–∞:")
                for i, case in enumerate(cases[:3], 1):
                    print(f"\n{i}. {case.get('case_number', 'N/A')}")
                    print(f"   –°—É–¥: {case.get('court', 'N/A')}")
                    print(f"   –î–∞—Ç–∞: {case.get('date', 'N/A')}")

        except Exception as e:
            logger.error("test_failed", error=str(e))
            raise


if __name__ == "__main__":
    print("‚ö†Ô∏è  –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Chrome –∑–∞–ø—É—â–µ–Ω —Å remote debugging!")
    print("   –ö–æ–º–∞–Ω–¥–∞: ./scripts/start_chrome_debug.sh\n")

    asyncio.run(test_parser_with_pagination())
